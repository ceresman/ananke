{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f3889a-d8ff-4f38-b2df-31db92ddbd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_{n+1}=a_{n}-\\frac{1}{c_{n}},b_{n+1}=b_{n}+\\frac{1}{d_{n}},c_{n+1}=c_{n}+\\frac{1}{a_{n+1}},d_{n+1}=d_{n}+\\frac{1}{b_{n+1}}.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pix2tex.cli import LatexOCR\n",
    "\n",
    "img = Image.open('math2.png')\n",
    "model = LatexOCR()\n",
    "print(model(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046d0c9-7ae7-4c3d-8cda-189d873a06cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af2f14d-0d38-469a-9ac3-65a9c8c857b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e922b-6238-4686-9bcb-c0a55aacaeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f344f85-16e0-4915-8c25-02a02dd6db18",
   "metadata": {},
   "source": [
    "![](./math2.png)\n",
    "$$\n",
    "a_{n+1}=a_{n}-\\frac{1}{c_{n}},b_{n+1}=b_{n}+\\frac{1}{d_{n}},c_{n+1}=c_{n}+\\frac{1}{a_{n+1}},d_{n+1}=d_{n}+\\frac{1}{b_{n+1}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d72582-0b0b-4fee-9d8d-a1627fa925c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ 2\\ll\\alpha f(\\alpha+\\varepsilon)+(\\alpha+\\varepsilon)f(\\alpha)\\mathop{\\bf\\Xi}\\le\\frac{\\alpha}{\\alpha+\\varepsilon}+\\left(\\alpha+\\varepsilon\\right)f(\\alpha)\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('math3.png')\n",
    "print(model(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183834b6-b952-4997-a67c-3c882a91249f",
   "metadata": {},
   "source": [
    "![](math3.png)\n",
    "$$\n",
    "\\ 2\\ll\\alpha f(\\alpha+\\varepsilon)+(\\alpha+\\varepsilon)f(\\alpha)\\mathop{\\bf\\Xi}\\le\\frac{\\alpha}{\\alpha+\\varepsilon}+\\left(\\alpha+\\varepsilon\\right)f(\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe1177-7774-4436-98ae-9f6b99b02a5f",
   "metadata": {},
   "source": [
    "$$\n",
    "2<x f(x+\\varepsilon)+(x+\\varepsilon) f(x) \\leq \\frac{x}{x+\\varepsilon}+(x+\\varepsilon) f(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3092585-ff7a-467d-a383-c90d03954d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def monotonic_activation(p):\n",
    "    return p  # Identity function for simplicity\n",
    "\n",
    "def grad_drop_backward(A, L_funcs, leak_params):\n",
    "    n = len(L_funcs)  # Number of tasks\n",
    "    grads = []\n",
    "    \n",
    "    # First, calculate the gradients for each task\n",
    "    for i in range(n):\n",
    "        G_i = torch.sign(A) * torch.autograd.grad(L_funcs[i](A), A, create_graph=True)[0]  # Equation for G_i\n",
    "        if G_i.requires_grad:\n",
    "            G_i = G_i.sum(dim=0)  # Sum over the batch dimension if required\n",
    "        grads.append(G_i)\n",
    "    \n",
    "    # Calculate P\n",
    "    abs_grad_sum = sum(torch.abs(g) for g in grads)\n",
    "    grad_sum = sum(grads)\n",
    "    P = 0.5 * (1 + grad_sum / abs_grad_sum)\n",
    "    \n",
    "    # Sample U, a tensor with the same shape as P\n",
    "    U = torch.rand_like(P)\n",
    "    \n",
    "    # Calculate masks and apply GradDrop\n",
    "    new_grad = 0\n",
    "    for i in range(n):\n",
    "        M_i = ((monotonic_activation(P) > U).float() * (grads[i] > 0).float()\n",
    "               + (monotonic_activation(P) < U).float() * (grads[i] < 0).float())\n",
    "        new_grad += (leak_params[i] + (1 - leak_params[i]) * M_i) * torch.autograd.grad(L_funcs[i](A), A, create_graph=True)[0]\n",
    "    \n",
    "    return new_grad\n",
    "\n",
    "# Example usage:\n",
    "# Assuming A is the input activation tensor, L_funcs is a list of loss functions, and leak_params is a list of leak parameters\n",
    "# A = torch.randn((batch_size, num_features), requires_grad=True)\n",
    "# L_funcs = [loss1, loss2, ..., lossN]  # Replace with actual loss functions\n",
    "# leak_params = [0.1, 0.2, ..., 0.1]  # Replace with actual leak parameters\n",
    "# new_grad = grad_drop_backward(A, L_funcs, leak_params)\n",
    "# Here, we would use new_grad to update the weights in the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc4dc06-e9cc-4daa-8b31-8374c2041c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight: Parameter containing:\n",
      "tensor([[ 0.1920, -0.0759, -0.3001, -0.1595,  0.1161],\n",
      "        [-0.3508,  0.3999, -0.0971, -0.0475,  0.3453],\n",
      "        [ 0.2742,  0.2636, -0.0997,  0.1964, -0.4187]], requires_grad=True)\n",
      "linear.bias: Parameter containing:\n",
      "tensor([ 0.3550, -0.0889, -0.1403], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network with a single linear layer\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Define the GradDrop hook\n",
    "def grad_drop_hook(grad, P, leak_param):\n",
    "    U = torch.rand_like(P)\n",
    "    M = ((monotonic_activation(P) > U).float() * (grad > 0).float()\n",
    "         + (monotonic_activation(P) < U).float() * (grad < 0).float())\n",
    "    return leak_param + (1 - leak_param) * M * grad\n",
    "\n",
    "# Test case setup\n",
    "input_size = 5\n",
    "output_size = 3\n",
    "batch_size = 2\n",
    "\n",
    "# Initialize the neural network\n",
    "net = SimpleNet(input_size, output_size)\n",
    "\n",
    "# Create some dummy input data\n",
    "x = torch.randn(batch_size, input_size)\n",
    "\n",
    "# Define two dummy loss functions for two different tasks\n",
    "def task1_loss(output):\n",
    "    target = torch.ones(batch_size, output_size)\n",
    "    return torch.nn.functional.mse_loss(output, target)\n",
    "\n",
    "def task2_loss(output):\n",
    "    target = torch.zeros(batch_size, output_size)\n",
    "    return torch.nn.functional.mse_loss(output, target)\n",
    "\n",
    "# List of loss functions\n",
    "L_funcs = [task1_loss, task2_loss]\n",
    "\n",
    "# Leak parameters for GradDrop\n",
    "leak_params = [0.0, 0.0]  # Pure GradDrop in this case\n",
    "\n",
    "# Forward pass through the network\n",
    "output = net(x)\n",
    "\n",
    "# Compute gradients for each task\n",
    "task_gradients = []\n",
    "for i, L_func in enumerate(L_funcs):\n",
    "    net.zero_grad()\n",
    "    loss = L_func(output)\n",
    "    loss.backward(retain_graph=True)\n",
    "    gradients = []\n",
    "    for p in net.parameters():\n",
    "        if p.grad is not None:\n",
    "            gradients.append(p.grad.clone())\n",
    "    task_gradients.append(gradients)\n",
    "\n",
    "# Apply GradDrop\n",
    "for i, p in enumerate(net.parameters()):\n",
    "    if p.requires_grad:\n",
    "        # Compute P for this parameter\n",
    "        abs_grad_sum = sum(torch.abs(g[i]) for g in task_gradients)\n",
    "        grad_sum = sum(g[i] for g in task_gradients)\n",
    "        P = 0.5 * (1 + grad_sum / abs_grad_sum)\n",
    "        \n",
    "        # Apply the GradDrop hook as a function since hooks cannot be used outside backward()\n",
    "        new_grad = grad_drop_hook(p.grad, P, leak_params[0])  # Assuming the same leak_param for simplicity\n",
    "        p.grad = new_grad\n",
    "\n",
    "# Update parameters\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "optimizer.step()\n",
    "\n",
    "# Print the updated weights and biases\n",
    "for name, param in net.named_parameters():\n",
    "    print(f\"{name}: {param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3064f0-e845-4836-8eb3-7debd862df3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
