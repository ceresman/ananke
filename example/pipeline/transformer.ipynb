{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f141e94a-4dc8-49a7-ba53-bce3bc8e9dc3",
   "metadata": {},
   "source": [
    "# All LLM Base On Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa8c7e-d102-4480-81eb-37b1aa60c56f",
   "metadata": {},
   "source": [
    "随着深度学习技术的发展，神经网络模型改变了自然语言处理的格局并提出了多个序列到序列（Sequence-to-Sequence）的基础模型架构，比如Transformer模型。凭借其强大的学习能力、并行计算的优势以及对长文本处理的能力，Transformer成为了现代大语言模型的基础架构。2020年，OpenAI发布了第一个大语言模型GPT-3[1]（Generative Pre-trained Transformer），引起了业界的广泛关注。GPT系列模型通过大规模无监督学习，在海量的文本数据中自动学习语言的特征和规律，从而可以生成和理解自然语言。这使得高效大语言模型不仅能够应对对话和翻译等传统任务，还可以创造性地生成新的文本内容，成为了多个领域的助手。然而，基于Transformer的大语言模型带来的挑战也不容忽视。其庞大的参数量和计算资源需求使得训练和部署成本变得极高；同时，模型的数据和计算复杂度也带来了隐私和安全的风险。\n",
    "\n",
    "- openai triton 的官方文档\n",
    "https://triton-lang.org/main/index.html  \n",
    "\n",
    "- lightllm 的 nopad \n",
    "https://github.com/ModelTC/lightllm/blob/main/lightllm/models/llama/triton_kernel/context_flashattention_nopad.py  \n",
    "\n",
    "- flashattention 实现\n",
    "https://github.com/Dao-AILab/flash-attention  flashattention 的官方仓库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba95198f-7ed4-4914-b426-cb0d322f67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabd59a-7e91-4ce2-b8aa-dc7e617f8e22",
   "metadata": {},
   "source": [
    "## word embedding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
