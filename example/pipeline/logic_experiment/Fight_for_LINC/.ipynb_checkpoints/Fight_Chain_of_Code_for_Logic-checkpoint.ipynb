{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0840ec1-eab8-4141-935e-c39a28e1aa08",
   "metadata": {},
   "source": [
    "# Construct Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77beaa44-f3be-4ada-bc5a-7ed5f18a3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.sem import logic\n",
    "from nltk.sem import Expression\n",
    "\n",
    "logic._counter._value = 0\n",
    "read_expr = Expression.fromstring\n",
    "prover = nltk.Prover9(10)\n",
    "\n",
    "\n",
    "def convert_to_nltk_rep(logic_formula):\n",
    "    translation_map = {\n",
    "        \"∀\": \"all \",\n",
    "        \"∃\": \"exists \",\n",
    "        \"→\": \"->\",\n",
    "        \"¬\": \"-\",\n",
    "        \"∧\": \"&\",\n",
    "        \"∨\": \"|\",\n",
    "        \"⟷\": \"<->\",\n",
    "        \"↔\": \"<->\",\n",
    "        \"0\": \"Zero\",\n",
    "        \"1\": \"One\",\n",
    "        \"2\": \"Two\",\n",
    "        \"3\": \"Three\",\n",
    "        \"4\": \"Four\",\n",
    "        \"5\": \"Five\",\n",
    "        \"6\": \"Six\",\n",
    "        \"7\": \"Seven\",\n",
    "        \"8\": \"Eight\",\n",
    "        \"9\": \"Nine\",\n",
    "        \".\": \"Dot\",\n",
    "        \"Ś\": \"S\",\n",
    "        \"ą\": \"a\",\n",
    "        \"’\": \"\",\n",
    "    }\n",
    "\n",
    "    constant_pattern = r'\\b([a-z]{2,})(?!\\()'\n",
    "    logic_formula = re.sub(constant_pattern, lambda match: match.group(1).capitalize(), logic_formula)\n",
    "\n",
    "    for key, value in translation_map.items():\n",
    "        logic_formula = logic_formula.replace(key, value)\n",
    "\n",
    "    quant_pattern = r\"(all\\s|exists\\s)([a-z])\"\n",
    "    def replace_quant(match):\n",
    "        return match.group(1) + match.group(2) + \".\"\n",
    "    logic_formula = re.sub(quant_pattern, replace_quant, logic_formula)\n",
    "\n",
    "    dotted_param_pattern = r\"([a-z])\\.(?=[a-z])\"\n",
    "    def replace_dotted_param(match):\n",
    "        return match.group(1)\n",
    "    logic_formula = re.sub(dotted_param_pattern, replace_dotted_param, logic_formula)\n",
    "\n",
    "    simple_xor_pattern = r\"(\\w+\\([^()]*\\)) ⊕ (\\w+\\([^()]*\\))\"\n",
    "    def replace_simple_xor(match):\n",
    "        return (\"((\" + match.group(1) + \" & -\" + match.group(2) + \") | (-\" + match.group(1) + \" & \" + match.group(2) + \"))\")\n",
    "    logic_formula = re.sub(simple_xor_pattern, replace_simple_xor, logic_formula)\n",
    "\n",
    "    complex_xor_pattern = r\"\\((.*?)\\)\\) ⊕ \\((.*?)\\)\\)\"\n",
    "    def replace_complex_xor(match):\n",
    "        return (\"(((\" + match.group(1) + \")) & -(\" + match.group(2) + \"))) | (-(\" + match.group(1) + \")) & (\" + match.group(2) + \"))))\")\n",
    "    logic_formula = re.sub(complex_xor_pattern, replace_complex_xor, logic_formula)\n",
    "\n",
    "    special_xor_pattern = r\"\\(\\(\\((.*?)\\)\\)\\) ⊕ (\\w+\\([^()]*\\))\"\n",
    "    def replace_special_xor(match):\n",
    "        return (\"(((\" + match.group(1) + \")) & -\" + match.group(2) + \") | (-(\" + match.group(1) + \")) & \" + match.group(2) + \")\")\n",
    "    logic_formula = re.sub(special_xor_pattern, replace_special_xor, logic_formula)\n",
    "    \n",
    "    return logic_formula\n",
    "\n",
    "def get_all_variables(text):\n",
    "    pattern = r'\\([^()]+\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    all_variables = []\n",
    "    for m in matches:\n",
    "        m = m[1:-1]\n",
    "        m = m.split(\",\")\n",
    "        all_variables += [i.strip() for i in m]\n",
    "    return list(set(all_variables))\n",
    "\n",
    "def reformat_fol(fol):\n",
    "    translation_map = {\n",
    "        \"0\": \"Zero\", \n",
    "        \"1\": \"One\",\n",
    "        \"2\": \"Two\",\n",
    "        \"3\": \"Three\",\n",
    "        \"4\": \"Four\",\n",
    "        \"5\": \"Five\",\n",
    "        \"6\": \"Six\",\n",
    "        \"7\": \"Seven\",\n",
    "        \"8\": \"Eight\",\n",
    "        \"9\": \"Nine\",\n",
    "        \".\": \"Dot\",\n",
    "        \"’\": \"\",\n",
    "        \"-\": \"_\",\n",
    "        \"'\": \"\",\n",
    "        \" \": \"_\"\n",
    "    }\n",
    "    all_variables = get_all_variables(fol)\n",
    "    for variable in all_variables:\n",
    "        variable_new = variable[:]\n",
    "        for k, v in translation_map.items():\n",
    "            variable_new = variable_new.replace(k, v)\n",
    "        fol = fol.replace(variable, variable_new)\n",
    "    return fol\n",
    "\n",
    "def evaluate(premises, conclusion):\n",
    "    premises = [reformat_fol(p) for p in premises]\n",
    "    conclusion = reformat_fol(conclusion)\n",
    "\n",
    "    c = read_expr(conclusion)\n",
    "    p_list = []\n",
    "    for p in premises:\n",
    "        p_list.append(read_expr(p))\n",
    "    truth_value = prover.prove(c, p_list)\n",
    "    if truth_value:\n",
    "        return \"True\"\n",
    "    else:\n",
    "        neg_c = read_expr(\"-(\" + conclusion + \")\")\n",
    "        negation_true = prover.prove(neg_c, p_list)\n",
    "        if negation_true:\n",
    "            return \"False\"\n",
    "        else:\n",
    "            return \"Uncertain\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93538007-bd27-48d9-b361-64c993085df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "INFILL_MODE = False\n",
    "\n",
    "\n",
    "class TokenizedDataset(IterableDataset):\n",
    "    \"\"\"Tokenize and preprocess the dataset\n",
    "    Multiple copies of the same prompt are sent sequentially.\n",
    "    See compute_code for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        task,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        num_devices,\n",
    "        max_length,\n",
    "        n_tasks=None,\n",
    "        n_copies=1,\n",
    "        prefix=\"\",\n",
    "    ):\n",
    "        self.task = task\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_devices = num_devices\n",
    "        self.max_length = max_length\n",
    "        self.n_tasks = n_tasks\n",
    "        self.n_copies = n_copies\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def __iter__(self):\n",
    "        prompts = []\n",
    "        infill = []\n",
    "        for sample in range(self.n_tasks):\n",
    "            prompt_contents = self.task.get_prompt(self.dataset[sample])\n",
    "            if isinstance(prompt_contents, str):\n",
    "                infill.append(False)\n",
    "                prompt = self.prefix + prompt_contents\n",
    "            elif isinstance(prompt_contents, dict):\n",
    "                assert set(prompt_contents.keys()) == {\"prefix\", \"suffix\"}\n",
    "                infill.append(True)\n",
    "                prompt = self.prefix + self._make_infill_prompt(**prompt_contents)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported prompt format: {type(prompt_contents)}\")\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        if not len(set(infill)) == 1:\n",
    "            raise ValueError(\"Mixed infill and completion prompts are not supported.\")\n",
    "        global INFILL_MODE\n",
    "        INFILL_MODE = infill[0]\n",
    "        if INFILL_MODE:\n",
    "            return_token_type_ids = False\n",
    "        else:\n",
    "            return_token_type_ids = None\n",
    "\n",
    "        outputs = self.tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=return_token_type_ids,\n",
    "        )\n",
    "\n",
    "        if self.n_copies == 1 and self.n_tasks % self.num_devices != 0:\n",
    "            self.n_copies = 2\n",
    "            warnings.warn(\n",
    "                \"n_copies (n_samples/batch_size) was changed from 1 to 2 because n_tasks isn't proportional to num devices\"\n",
    "            )\n",
    "\n",
    "        for sample in range(self.n_tasks):\n",
    "            for _ in range(self.n_copies):\n",
    "                yield {\n",
    "                    \"ids\": outputs.input_ids[sample],\n",
    "                    \"task_id\": sample,\n",
    "                    \"input_len\": outputs.attention_mask[sample].sum(),\n",
    "                }\n",
    "\n",
    "    def _make_infill_prompt(self, prefix, suffix):\n",
    "        \"\"\"Make a prompt for infilling.\n",
    "        Currently supported only for official InCoder and SantaCoder implementations.\n",
    "        \"\"\"\n",
    "        model_id = self.tokenizer.name_or_path\n",
    "        if model_id in [\"facebook/incoder-1B\", \"facebook/incoder-6B\"]:\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            return f\"{prefix}<|mask:0|>{suffix}<|mask:0|>\"\n",
    "        elif model_id in [\"bigcode/santacoder\"]:\n",
    "            return f\"<fim-prefix>{prefix}<fim-suffix>{suffix}<fim-middle>\"\n",
    "        else:\n",
    "            raise ValueError(f\"Infilling not yet supported for: {model_id}\")\n",
    "\n",
    "\n",
    "def complete_code(\n",
    "    task,\n",
    "    accelerator,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataloader,\n",
    "    n_tasks,\n",
    "    batch_size=20,\n",
    "    prefix=\"\",\n",
    "    postprocess=True,\n",
    "    **gen_kwargs,\n",
    "):\n",
    "    \"\"\"Generate multiple codes for each task in the dataset using multiple GPUs with accelerate.\n",
    "    dataloader sends all the prompts from the evalution dataset to the model as the following:\n",
    "    [p_0_0, p_0_1, ..., p_0_nc-1, p_1_0, ..., p_nt-1_nc-1] where nc is the number of copies of the prompt,\n",
    "    and nt is the number of tasks. nc is such that num_samples(for each task)= nc * batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    gen_token_dict = defaultdict(list)\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=math.ceil(\n",
    "            n_tasks * dataloader.dataset.n_copies / accelerator.num_processes\n",
    "        ),\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            if task.stop_words:\n",
    "                gen_kwargs[\"stopping_criteria\"][0].start_length = batch[\"ids\"].shape[-1]\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                input_ids=batch[\"ids\"][:, : batch[\"input_len\"]],\n",
    "                num_return_sequences=batch_size,\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "            generated_tasks = batch[\"task_id\"].repeat(batch_size)\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens, generated_tasks = accelerator.gather(\n",
    "                (generated_tokens, generated_tasks)\n",
    "            )\n",
    "            generated_tokens = generated_tokens.cpu().numpy()\n",
    "            generated_tasks = generated_tasks.cpu().numpy()\n",
    "\n",
    "            for sample, generated_tokens in zip(generated_tasks, generated_tokens):\n",
    "                gen_token_dict[sample].append(generated_tokens)\n",
    "\n",
    "    def parse_infill(code, tokenizer):\n",
    "        \"\"\"Reorder infill code and remove remaining special tokens.\"\"\"\n",
    "        model_id = tokenizer.name_or_path\n",
    "        if model_id in [\"facebook/incoder-1B\", \"facebook/incoder-6B\"]:\n",
    "            prefix, suffix, infill = code.split(\"<|mask:0|>\", 2)\n",
    "            infill = infill.split(\"<|endofmask|>\")[0]\n",
    "        elif model_id in [\"bigcode/santacoder\"]:\n",
    "            prefix, rest = code.split(\"<fim-suffix>\", 1)\n",
    "            suffix, infill = rest.split(\"<fim-middle>\", 1)\n",
    "            infill = infill.split(\"<|endoftext|>\")[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Infilling not yet supported for: {model_id}\")\n",
    "        code = \"\".join([prefix, infill, suffix])\n",
    "        for k, v in tokenizer.special_tokens_map.items():\n",
    "            if k == \"additional_special_tokens\":\n",
    "                for t in v:\n",
    "                    code = code.replace(t, \"\")\n",
    "            else:\n",
    "                code = code.replace(v, \"\")\n",
    "        return code\n",
    "\n",
    "    code_gens_raw = [[] for _ in range(n_tasks)]\n",
    "    code_gens_prc = [[] for _ in range(n_tasks)]\n",
    "    for sample, generated_tokens in gen_token_dict.items():\n",
    "        for s in generated_tokens:\n",
    "            if INFILL_MODE:\n",
    "                gen_code = parse_infill(\n",
    "                    tokenizer.decode(\n",
    "                        s, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "                    ),\n",
    "                    tokenizer,\n",
    "                )\n",
    "            else:\n",
    "                gen_code = tokenizer.decode(\n",
    "                    s, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            code_gens_raw[sample].append(gen_code[len(prefix) :])\n",
    "            if postprocess:\n",
    "                code_gens_prc[sample].append(\n",
    "                    task.postprocess_generation(gen_code[len(prefix) :], int(sample))\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    \"model output is not postprocessed, this might lower evaluation scores\"\n",
    "                )\n",
    "                code_gens_prc[sample].append(gen_code[len(prefix) :])\n",
    "\n",
    "    return code_gens_prc, code_gens_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "905efb86-9bfe-4d09-8004-133d7b212de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "from collections import Counter\n",
    "# from eval.tasks.utils import evaluate, convert_to_nltk_rep\n",
    "from abc import abstractmethod, ABC\n",
    "from datasets import load_dataset, Dataset\n",
    "from warnings import warn\n",
    "\n",
    "import pdb\n",
    "class Task(ABC):\n",
    "    \"\"\"A task represents an entire benchmark including its dataset, problems,\n",
    "    answers, generation settings and evaluation methods.\n",
    "    \"\"\"\n",
    "\n",
    "    # The name of the `Task` benchmark as denoted in the HuggingFace datasets Hub\n",
    "    DATASET_PATH: str = None\n",
    "\n",
    "    # The name of a subset within `DATASET_PATH`.\n",
    "    DATASET_NAME: str = None\n",
    "\n",
    "    def __init__(self, stop_words=None, requires_execution=True):\n",
    "        \"\"\"\n",
    "        :param stop_words: list\n",
    "            list of stop words if the generation uses a stopping criteria during generation\n",
    "        :param requires_execution: bool\n",
    "            wheter the task requires code execution during evaluation or not\n",
    "        \"\"\"\n",
    "        self.stop_words = stop_words\n",
    "        self.requires_execution = requires_execution\n",
    "        # pdb.set_trace()\n",
    "        try:\n",
    "            self.dataset = load_dataset(path=self.DATASET_PATH, name=self.DATASET_NAME)\n",
    "        except:\n",
    "            warn(\n",
    "                \"This task will use a locally downloaded dataset, not from the HF hub.\"\n",
    "            )\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_dataset(self):\n",
    "        \"\"\"Returns dataset for the task or an iterable of any object, that get_prompt can handle\"\"\"\n",
    "        return []\n",
    "\n",
    "    def fewshot_examples(self):\n",
    "        \"\"\"Loads and returns the few-shot examples for the task if they exist.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_prompt(self, doc):\n",
    "        \"\"\"Builds the prompt for the LM to generate from.\n",
    "        :param doc: dict[str: str]\n",
    "            sample from the test dataset\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_reference(self, doc):\n",
    "        \"\"\"Builds the reference solution for the doc.\n",
    "        :param doc: dict[str: str]\n",
    "            sample from the test dataset\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def postprocess_generation(self, generation, idx):\n",
    "        \"\"\"Defines the postprocessing for a LM generation.\n",
    "        :param generation: str\n",
    "            code generation from LM\n",
    "        :param idx: int\n",
    "            index of doc in the dataset to which the generation belongs\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_results(self, generations, references):\n",
    "        \"\"\"Takes the list of LM generations and evaluates them against ground truth references,\n",
    "        returning the metric for the generations as in {\"metric_name\": result}.\n",
    "        :param generations: list(list(str))\n",
    "            list of lists containing generations\n",
    "        :param references: list(str)\n",
    "            list of str containing refrences\n",
    "        :return: dict[str: float]\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "import pdb\n",
    "\n",
    "class OWAFOLTask(Task):\n",
    "    \"\"\"An OWA (Open World Assumption) FOL (First Order Logic) Task is a Task in which the goal\n",
    "    is to generate True/False/Uncertain answers to First Order Logic questions.\n",
    "    \"\"\"\n",
    "\n",
    "    TRAIN_DATASET_PATH = \"metaeval/folio\"\n",
    "    ERROR_TOKEN = \"Error\"\n",
    "    MAX_SHOTS = 16\n",
    "\n",
    "    def __init__(self, mode, n):\n",
    "        assert n <= self.MAX_SHOTS, f\"supports up to {self.MAX_SHOTS}-shot\"\n",
    "        super().__init__(\n",
    "            stop_words=[\"</EVALUATE>\"], requires_execution=True,\n",
    "        )\n",
    "        self._mode = mode\n",
    "        self._nshot = n\n",
    "        # pdb.set_trace()\n",
    "        self.train_dataset = Dataset.load_from_disk('/workspace/hyf/ananke/example/pipeline/logic_experiment/Fight_for_LINC/Chain-of-Context/metaeval_folio/train')#load_dataset(self.TRAIN_DATASET_PATH)[\"train\"]\n",
    "        self._train_dataset = self.reformat_fol_samples_train(self.train_dataset)\n",
    "        self._train_dataset = self.add_conclusion_fols_train(self._train_dataset)\n",
    "        self._train_dataset = self.add_cot_train(self._train_dataset)\n",
    "        self._train_dataset = self.add_new_premises_from_cot(self._train_dataset)\n",
    "        self._train_dataset = self._train_dataset.map(\n",
    "            lambda x: {\"label\": \"Uncertain\" if x[\"label\"] == \"Unknown\" else x[\"label\"]},\n",
    "            remove_columns=[\"label\"],\n",
    "        )\n",
    "        self._train_fewshot_indices_all = [\n",
    "            125,\n",
    "            23,\n",
    "            60,\n",
    "            275,\n",
    "            148,\n",
    "            261,\n",
    "            263,\n",
    "            683,\n",
    "            299,\n",
    "            684,\n",
    "            850,\n",
    "            853,\n",
    "            886,\n",
    "            892,\n",
    "            930,\n",
    "            980,\n",
    "        ]\n",
    "        # Labels:\n",
    "        # 23 (True), 60 (False), 125 (Uncertain), 148 (False), 261 (True), 263 (True), 275 (Uncertain), 683 (Uncertain)\n",
    "        # 299 (True), 684 (False), 850 (False), 853 (Uncertain), 886 (True), 892 (Uncertain), 930 (False), 980 (False)\n",
    "\n",
    "        self._train_fewshot_indices = self._train_fewshot_indices_all[:n]\n",
    "        self._train = self._train_dataset.select(self._train_fewshot_indices)\n",
    "\n",
    "    def reformat_fol_samples_train(self, train_dataset):\n",
    "        def reformat_fol_sample(sample):\n",
    "            sample[\"premises-FOL\"] = [\n",
    "                convert_to_nltk_rep(premise) for premise in sample[\"premises-FOL\"]\n",
    "            ]\n",
    "            return sample\n",
    "\n",
    "        return train_dataset.map(reformat_fol_sample)\n",
    "\n",
    "    def add_conclusion_fols_train(self, train_dataset):\n",
    "        train_conclusion_fols = {\n",
    "            23: \"HigherRank(RealMadrid, Barcelona)\",\n",
    "            60: \"-OlympicGoldMedalWinner(Amy) -> NobelLaureate(Amy)\",\n",
    "            125: \"-Dispensable(Worksheet)\",\n",
    "            148: \"FolkSong(Inception)\",\n",
    "            261: \"MakeGoodBreakfast(Luke)\",\n",
    "            263: \"exists x. (Develops(Ets, x) & For(x, k-OneTwoandhighereducation)) & exists x. (Develops(Ets, x) & AssociatedWith(x, Entrytouseducationinstitutions))\",\n",
    "            275: \"ContributeToCountry(James)\",\n",
    "            299: \"GetRhythmRight(John)\",\n",
    "            683: \"exists x. (BRICS(x) & Speaks(x, Hindi))\",\n",
    "            684: \"Film(Hamilton)\",\n",
    "            850: \"-Liked(Leo, Charlie) & -Cares(Charlie, Leo)\",\n",
    "            853: \"Won(Threebodyproblem, Hugoaward)\",\n",
    "            886: \"Dagfinn(DagfinnAarskog)\",\n",
    "            892: \"PartOf(Minsk, Scottishpremiership)\",\n",
    "            930: \"-Locate(Boves, Europe)\",\n",
    "            980: \"(InvitedTakePhoto(James) & -HappyCommunicate(James)) | (-InvitedTakePhoto(James) & HappyCommunicate(James))\",\n",
    "        }\n",
    "        conclusions = [None for _ in range(len(train_dataset))]\n",
    "        for index, conclusion_fol in train_conclusion_fols.items():\n",
    "            conclusions[index] = conclusion_fol\n",
    "        train_dataset = train_dataset.add_column(\"conclusion-FOL\", conclusions)\n",
    "        return train_dataset\n",
    "\n",
    "    def add_cot_train(self, train_dataset):\n",
    "        train_cots = {\n",
    "            23: \"Let's think step by step. We want to evaluate if in La Liga 2021-2022, Real Madrid ranks higher than Barcelona. From premise 1, we know that a La Liga soccer team ranks higher than another if it receives more points. From premise 4, we know that in La Liga 2021-2022, Real Madrid received more points than Barcelona. Therefore, in La Liga 2021-2022, Real Madrid received more points than Barcelona, so Real Madrid ranks higher than Barcelona, so the statement is true.\\nANSWER:\\tTrue\",\n",
    "            60: \"Let's think step by step. We want to evaluate the statement \\\"if Amy is not an Olympic gold medal winner, then Amy is a Nobel laureate\\\". Let's assume that Amy is not an Olympic gold medal winner. This doesn't tell us anything about whether Amy is a Nobel laureate, so the statement isn't true, meaning it is either False or Uncertain. To distinguish between the two, notice that we could have a scenario where Amy is neither an Olympic gold medal winner nor a Nobel laureate. None of the premises are violated in this case. This means the statement must be false.\\nANSWER:\\tFalse\",\n",
    "            125: \"Let's think step by step. We want to evaluate if a worksheet is not dispensable. From premise 6, we know that a worksheet is either paper or is environment-friendly. If it is paper, then from premise 3, a worksheet is woodware, and from premise 2, a worksheet is dispensable. If it is environment-friendly, we know it is good from premise 5, but we know nothing about whether it is dispensable. Therefore, we don't know if a worksheet is dispensible or not, so the statement is uncertain.\\nANSWER:\\tUncertain\",\n",
    "            148: \"Let's think step by step. We want to evaluate if Inception is a folk song. We know that Inception is a sci-fi movie. Since all movies are videos and Inception is a movie, it is a video, which means it is visual. On the other hand, we know that all folk songs are songs, and no songs are visual, so no folk songs are visual. Therefore, since Inception is visual but no folk songs are visual, we know that Inception cannot be a folk song, so the statement is false.\\nANSWER:\\tFalse\",\n",
    "            261: \"Let's think step by step. We want to evaluate if Luke can make a good breakfast. From the last premise, we know that Luke can make cookies, scrambled eggs, and muffins. Since Luke can make cookies and muffins, they are a baker. Now, combining the information we have, since Luke is a baker and can make scrambled eggs, this means that they can make a good breakfast. Therefore, Luke can make a good breakfast, so the statement is true.\\nANSWER:\\tTrue\",\n",
    "            263: \"Let's think step by step. We want to evaluate if ETS develops assessments for K-12 statewide as well as entry to US tertiary and quaternary educatiand doon institutions. We know that ETS develops assessments for K-12 statewide. We also know that ETS develops assessments associated with entry to the US tertiary and quaternary education institutes. Therefore, both parts of the conclusion are true, and the statement is true.\\nANSWER:\\tTrue\",\n",
    "            275: \"Let's think step by step. We want to evaluate if James contributes to the country. Let's think about what we know about James. First, we know that James was either sentenced for thief or stayed in prison. However, this doesn't tell us anything about whether James contributed to the country. Second, we know that James either had a bad record in the local state or that he was respected by others. However, the premises don't tell us anything about the relationship between having a bad record and contributing to the country. Therefore, it is uncertain whether James contributes to the country.\\nANSWER:\\tUncertain\",\n",
    "            299: \"Let's think step by step. We want to evaluate if John can get the rhythms right. We know that John is a student learning piano. Since all students learning piano can strike the right notes, John can strike the right notes. Since all students who can strike the right notes can get the rhythms right and John can strike the right notes, John can get the rhythms right, so the conclusion is true.\\nANSWER:\\tTrue\",\n",
    "            683: \"Let's think step by step. We want to evaluate if there is a person from BRICS speaking Hindi. We know that there is an Indian, and since India is one of BRICS, we know that there is an Indian in BRICS. Furthermore, we know that they speak either Hindi or English, however, we don't know which one. Therefore, there could be a person in BRICS speaking Hindi, or there could not. Therefore, it is uncertain whether there is a person from BRICS speaking Hindi.\\nANSWER:\\tUncertain\",\n",
    "            684: \"Let's think step by step. We want to evaluate if Hamilton is a film. Since Daveed Diggs played two roles in the musical Hamilton, Hamilton is a musical. Since musicals are not films and Hamilton is a musical, Hamilton is not a film, and the conclusion is false.\\nANSWER:\\tFalse\",\n",
    "            850: \"Let's think step by step. We want to evaluate if Charlie does not like Leo and does not care for Leo. Let's first evaluate if Charlie does not like Leo. We know Charlie has a naughty pet named Leo. Since pets who are naughty are not liked as much, Charlie does not like Leo. Now, let's evaluate if Charlie cares for Leo. We know that if a person has a pet, they care for that pet. Since Leo is Charlie's pet, Charlie cares for Leo. Therefore, Charlie does not like Leo but cares for Leo, so the second part of the conclusion is false, which means the entire conclusion is false.\\nANSWER:\\tFalse\",\n",
    "            853: \"Let's think step by step. We want to evaluate if the Three Body Problem won the Hugo Award. The only thing we know about the Hugo Award is that some books that have won the Hugo Award were written by Cixin Liu. However, we know nothing about whether The Three Body Problem was written by Cixin Liu, so the conclusion is uncertain.\\nANSWER:\\tUncertain\",\n",
    "            886: \"Let's think step by step. We want to evaluate if Dagfinn is Dagfinn Aarskog's given name. We know that Dagfinn is a given name, and that notable people with the given name Dagfinn includes Dagfinn Aarskog, which means that Dagfinn is Dagfinn Aarskog's given name, so the conclusion is true.\\nANSWER:\\tTrue\",\n",
    "            892: \"Let's think step by step. We want to evaluate if Minsk joined the Scottish Premiership. We know that Minsk and St Johnstone are different teams and that St Johnstone is part of the Scottish Premiership, but we don't know anything about whether or not Minsk joined the Scottish Premiership from the premises. Therefore, the conclusion is uncertain.\\nANSWER:\\tUncertain\",\n",
    "            930: \"Let's think step by step. We want to evaluate if Boves is not in Europe. We know that Boves is a railway station located in France. We also know that since France is a European country, France is located in Europe. Furthermore, we know that if A is located in B and B is located in C, then A is located in C. Therefore, we know that because Boves is located in France and France is located in Europe, that means Boves is located in Europe. Therefore, the conclusion is false.\\nANSWER:\\tFalse\",\n",
    "            980: \"Let's think step by step. We want to evaluate if James is either invited to take a photo with the audience or happy to communicate with each other during the dinner. We know that James does not attend the conference in person and is not provided with souvenirs. There are no premises that apply to people who do not attend the conference. Since James is not provided with souvenirs, since all who attended the conference in person are provided with souvenirs, we know that James did not attend the conference in person. However, we don't know anything else, so it is possible that James was neither invited to take a photo with the audience nor happy to communicate during the dinner. Therefore, the conclusion is false.\\nANSWER:\\tFalse\",\n",
    "        }\n",
    "        cots = [None for _ in range(len(train_dataset))]\n",
    "        for index, cot in train_cots.items():\n",
    "            cots[index] = cot\n",
    "        train_dataset = train_dataset.add_column(\"cot\", cots)\n",
    "        return train_dataset\n",
    "\n",
    "    def add_new_premises_from_cot(self, train_dataset):\n",
    "        train_new_premises_from_cots = {\n",
    "            23: \"Let's think step by step. We want to evaluate if in La Liga 2021-2022, Real Madrid ranks higher than Barcelona. From premise 1, we know that a La Liga soccer team ranks higher than another if it receives more points. From premise 4, we know that in La Liga 2021-2022, Real Madrid received more points than Barcelona. Therefore, in La Liga 2021-2022, Real Madrid received more points than Barcelona, so Real Madrid ranks higher than Barcelona, so the statement is true.\\nANSWER:\\tTrue\",\n",
    "            60: \"Let's think step by step. We want to evaluate the statement \\\"if Amy is not an Olympic gold medal winner, then Amy is a Nobel laureate\\\". Let's assume that Amy is not an Olympic gold medal winner. This doesn't tell us anything about whether Amy is a Nobel laureate, so the statement isn't true, meaning it is either False or Uncertain. To distinguish between the two, notice that we could have a scenario where Amy is neither an Olympic gold medal winner nor a Nobel laureate. None of the premises are violated in this case. This means the statement must be false.\\nANSWER:\\tFalse\",\n",
    "            125: \"Let's think step by step. We want to evaluate if a worksheet is not dispensable. From premise 6, we know that a worksheet is either paper or is environment-friendly. If it is paper, then from premise 3, a worksheet is woodware, and from premise 2, a worksheet is dispensable. If it is environment-friendly, we know it is good from premise 5, but we know nothing about whether it is dispensable. Therefore, we don't know if a worksheet is dispensible or not, so the statement is uncertain.\\nANSWER:\\tUncertain\",\n",
    "            148: \"Let's think step by step. We want to evaluate if Inception is a folk song. We know that Inception is a sci-fi movie. Since all movies are videos and Inception is a movie, it is a video, which means it is visual. On the other hand, we know that all folk songs are songs, and no songs are visual, so no folk songs are visual. Therefore, since Inception is visual but no folk songs are visual, we know that Inception cannot be a folk song, so the statement is false.\\nANSWER:\\tFalse\",\n",
    "            261: \"Let's think step by step. We want to evaluate if Luke can make a good breakfast. From the last premise, we know that Luke can make cookies, scrambled eggs, and muffins. Since Luke can make cookies and muffins, they are a baker. Now, combining the information we have, since Luke is a baker and can make scrambled eggs, this means that they can make a good breakfast. Therefore, Luke can make a good breakfast, so the statement is true.\\nANSWER:\\tTrue\",\n",
    "            263: \"Let's think step by step. We want to evaluate if ETS develops assessments for K-12 statewide as well as entry to US tertiary and quaternary educatiand doon institutions. We know that ETS develops assessments for K-12 statewide. We also know that ETS develops assessments associated with entry to the US tertiary and quaternary education institutes. Therefore, both parts of the conclusion are true, and the statement is true.\\nANSWER:\\tTrue\",\n",
    "            275: \"Let's think step by step. We want to evaluate if James contributes to the country. Let's think about what we know about James. First, we know that James was either sentenced for thief or stayed in prison. However, this doesn't tell us anything about whether James contributed to the country. Second, we know that James either had a bad record in the local state or that he was respected by others. However, the premises don't tell us anything about the relationship between having a bad record and contributing to the country. Therefore, it is uncertain whether James contributes to the country.\\nANSWER:\\tUncertain\",\n",
    "            299: \"Let's think step by step. We want to evaluate if John can get the rhythms right. We know that John is a student learning piano. Since all students learning piano can strike the right notes, John can strike the right notes. Since all students who can strike the right notes can get the rhythms right and John can strike the right notes, John can get the rhythms right, so the conclusion is true.\\nANSWER:\\tTrue\",\n",
    "            683: \"Let's think step by step. We want to evaluate if there is a person from BRICS speaking Hindi. We know that there is an Indian, and since India is one of BRICS, we know that there is an Indian in BRICS. Furthermore, we know that they speak either Hindi or English, however, we don't know which one. Therefore, there could be a person in BRICS speaking Hindi, or there could not. Therefore, it is uncertain whether there is a person from BRICS speaking Hindi.\\nANSWER:\\tUncertain\",\n",
    "            684: \"Let's think step by step. We want to evaluate if Hamilton is a film. Since Daveed Diggs played two roles in the musical Hamilton, Hamilton is a musical. Since musicals are not films and Hamilton is a musical, Hamilton is not a film, and the conclusion is false.\\nANSWER:\\tFalse\",\n",
    "            850: \"Let's think step by step. We want to evaluate if Charlie does not like Leo and does not care for Leo. Let's first evaluate if Charlie does not like Leo. We know Charlie has a naughty pet named Leo. Since pets who are naughty are not liked as much, Charlie does not like Leo. Now, let's evaluate if Charlie cares for Leo. We know that if a person has a pet, they care for that pet. Since Leo is Charlie's pet, Charlie cares for Leo. Therefore, Charlie does not like Leo but cares for Leo, so the second part of the conclusion is false, which means the entire conclusion is false.\\nANSWER:\\tFalse\",\n",
    "            853: \"Let's think step by step. We want to evaluate if the Three Body Problem won the Hugo Award. The only thing we know about the Hugo Award is that some books that have won the Hugo Award were written by Cixin Liu. However, we know nothing about whether The Three Body Problem was written by Cixin Liu, so the conclusion is uncertain.\\nANSWER:\\tUncertain\",\n",
    "            886: \"Let's think step by step. We want to evaluate if Dagfinn is Dagfinn Aarskog's given name. We know that Dagfinn is a given name, and that notable people with the given name Dagfinn includes Dagfinn Aarskog, which means that Dagfinn is Dagfinn Aarskog's given name, so the conclusion is true.\\nANSWER:\\tTrue\",\n",
    "            892: \"Let's think step by step. We want to evaluate if Minsk joined the Scottish Premiership. We know that Minsk and St Johnstone are different teams and that St Johnstone is part of the Scottish Premiership, but we don't know anything about whether or not Minsk joined the Scottish Premiership from the premises. Therefore, the conclusion is uncertain.\\nANSWER:\\tUncertain\",\n",
    "            930: \"Let's think step by step. We want to evaluate if Boves is not in Europe. We know that Boves is a railway station located in France. We also know that since France is a European country, France is located in Europe. Furthermore, we know that if A is located in B and B is located in C, then A is located in C. Therefore, we know that because Boves is located in France and France is located in Europe, that means Boves is located in Europe. Therefore, the conclusion is false.\\nANSWER:\\tFalse\",\n",
    "            980: \"Let's think step by step. We want to evaluate if James is either invited to take a photo with the audience or happy to communicate with each other during the dinner. We know that James does not attend the conference in person and is not provided with souvenirs. There are no premises that apply to people who do not attend the conference. Since James is not provided with souvenirs, since all who attended the conference in person are provided with souvenirs, we know that James did not attend the conference in person. However, we don't know anything else, so it is possible that James was neither invited to take a photo with the audience nor happy to communicate during the dinner. Therefore, the conclusion is false.\\nANSWER:\\tFalse\",\n",
    "        }\n",
    "        \n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"Returns dataset for the task or an iterable of any object, that get_prompt can handle\"\"\"\n",
    "        return self._test\n",
    "\n",
    "    def get_instructions(self):\n",
    "        instructions = \"\"\n",
    "        instructions += \"The following is a first-order logic (FOL) problem.\\n\"\n",
    "        instructions += \"The problem is to determine whether the conclusion follows from the premises.\\n\"\n",
    "        instructions += \"The premises are given in the form of a set of first-order logic sentences.\\n\"\n",
    "        instructions += \"The conclusion is given in the form of a single first-order logic sentence.\\n\"\n",
    "        if self._mode == \"baseline\":\n",
    "            instructions += f\"The task is to evaluate the conclusion as 'True', 'False', or 'Uncertain' given the premises.\"\n",
    "        else:\n",
    "            instructions += \"The task is to translate each of the premises and conclusions into FOL expressions, \"\n",
    "            if self._mode == \"scratchpad\":\n",
    "                instructions += f\"and then to evaluate the conclusion as 'True', 'False', or 'Uncertain' given the premises.\"\n",
    "            elif self._mode == \"neurosymbolic\":\n",
    "                instructions += \"so that the expressions can be evaluated by a theorem solver to determine whether the conclusion follows from the premises.\\n\"\n",
    "                instructions += \"Expressions should be adhere to the format of the Python NLTK package logic module.\"\n",
    "        return instructions + \"\\n\\n\"\n",
    "\n",
    "    def format_train_example(self, doc):\n",
    "        example = self.format_test_example(doc)\n",
    "        if self._mode == \"baseline\":\n",
    "            example += f\"{doc['label'].strip()}\\n\"\n",
    "        elif self._mode == \"cot\":\n",
    "            example += f\"{doc['cot']}\\n\"\n",
    "        else:\n",
    "            # for premise, fol in zip(doc[\"premises\"], doc[\"premises-FOL\"]):\n",
    "            #     example += f\"TEXT:\\t{premise.strip()}\\nFOL:\\t{fol.strip()}\\n\"\n",
    "            # example += f\"TEXT:\\t{doc['conclusion'].strip()}\\nFOL:\\t{doc['conclusion-FOL'].strip()}\\n\"\n",
    "            # if self._mode == \"scratchpad\":\n",
    "            #     example += f\"ANSWER:\\t{doc['label'].strip()}\\n\"\n",
    "            example += f\"{doc['cot']}\\n\"\n",
    "            example += \"from the above chain of thought, we can obtain the following new premises and corresponding FOL expressions:\"\n",
    "            example += \"<NEW PREMISES>\\n\"\n",
    "            for premise in doc[\"new_premises\"], doc[\"new-premises-FOL\"]):\n",
    "                example += f\"TEXT:\\t{premise.strip()}\\nFOL:\\t{fol.strip()}\\n\"\n",
    "            example += \"</NEW PREMISES>\\n\"\n",
    "            example += \"And we can obtain that the relation between premises and conclusion is :\"\n",
    "            example += \"<COT ANSWER>\\n\"\n",
    "            f\"ANSWER:\\t{doc['label'].strip()}\\n\"\n",
    "            example += \"<\\COT ANSWER>\\n\"\n",
    "            example += \"Moreover we can have original premises, and their corresponding FOL expressions as :\"\n",
    "            for premise, fol in zip(doc[\"premises\"], doc[\"premises-FOL\"]):\n",
    "                example += f\"TEXT:\\t{premise.strip()}\\nFOL:\\t{fol.strip()}\\n\"\n",
    "            example += \"And for the conclusion and its FOL expression:\"\n",
    "            example += f\"TEXT:\\t{doc['conclusion'].strip()}\\nFOL:\\t{doc['conclusion-FOL'].strip()}\\n\"\n",
    "        return example + \"</EVALUATE>\\n\"\n",
    "\n",
    "    def format_test_example(self, doc):\n",
    "        example = \"<PREMISES>\\n\"\n",
    "        for premise in doc[\"premises\"]:\n",
    "            example += f\"{premise.strip()}\\n\"\n",
    "        example += \"</PREMISES>\\n\"\n",
    "        example += f\"<CONCLUSION>\\n{doc['conclusion'].strip()}\\n</CONCLUSION>\\n\"\n",
    "        example += \"<EVALUATE>\\n\"\n",
    "        return example\n",
    "\n",
    "    def get_prompt(self, doc):\n",
    "        \"\"\"\n",
    "        Builds the prompt for the LM to generate from.\n",
    "        :param doc: dict[str: str]\n",
    "            sample from the test dataset\n",
    "        :return: str\n",
    "        \"\"\"\n",
    "        instructions = self.get_instructions()\n",
    "        train = self.fewshot_examples()\n",
    "        test = self.format_test_example(doc)\n",
    "        prompt = \"\\n\".join([instructions, train, test])\n",
    "        return prompt\n",
    "\n",
    "    def get_reference(self, doc):\n",
    "        \"\"\"\n",
    "        Builds the reference solution for the doc (sample from the test dataset).\n",
    "        :param doc: dict[str: str]\n",
    "            sample from the test dataset\n",
    "        :return: str\n",
    "        \"\"\"\n",
    "        return doc[\"label\"]\n",
    "\n",
    "    def postprocess_generation(self, generation, idx, completion_only=False):\n",
    "        \"\"\"\n",
    "        Defines the postprocessing for a LM generation.\n",
    "        :param generation: str\n",
    "            code generation from LM\n",
    "        :param idx: int (if needed)\n",
    "            index of doc in the dataset to which the generation belongs\n",
    "        :return: str\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if completion_only:\n",
    "                gen = generation.strip()\n",
    "            else:\n",
    "                prefix = self.get_prompt(self.get_dataset()[idx])\n",
    "                assert generation.startswith(\n",
    "                    prefix\n",
    "                ), \"Increase `--max_length_generation` to avoid truncation\"\n",
    "                gen = generation[len(prefix) :].strip()\n",
    "                for stop_word in self.stop_words:\n",
    "                    gen = gen.split(stop_word)[0].strip()\n",
    "            if self._mode == \"baseline\":\n",
    "                resp = gen.strip()\n",
    "            elif self._mode == \"scratchpad\":\n",
    "                flag = \"ANSWER:\"\n",
    "                resp = gen.split(flag)[-1].strip()\n",
    "            elif self._mode == \"neurosymbolic\":\n",
    "                flag = \"FOL:\"\n",
    "                parses = [\n",
    "                    line.replace(flag, \"\").strip()\n",
    "                    for line in gen.split(\"\\n\")\n",
    "                    if flag in line\n",
    "                ]\n",
    "                premises, conclusion = parses[:-1], parses[-1]\n",
    "                resp = evaluate(premises, conclusion)\n",
    "            elif self._mode == \"cot\":\n",
    "                flag = \"ANSWER:\"\n",
    "                resp = gen.split(flag)[-1].strip()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mode: {self._mode}\")\n",
    "            assert resp in [\"True\", \"False\", \"Uncertain\"], f\"Invalid generation: {resp}\"\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            # TODO: explore failure cases and improve postprocessing\n",
    "            print(f\"Error in parsing and/or evaluating LLM output: {e}\")\n",
    "            return self.ERROR_TOKEN\n",
    "\n",
    "    @staticmethod\n",
    "    def metric(generations, references, error_token):\n",
    "        correct = 0\n",
    "        for gens, ref in zip(generations, references):\n",
    "            gens = [gen for gen in gens if gen != error_token]\n",
    "            if len(gens) > 0:\n",
    "                majority = Counter(gens).most_common(1)[0][0]\n",
    "                if majority == ref:\n",
    "                    correct += 1\n",
    "        return {f\"accuracy (pass@1 majority)\": correct / len(references)}\n",
    "\n",
    "    def process_results(self, generations, references):\n",
    "        \"\"\"\n",
    "        Takes the list of LM generations and evaluates them against ground truth references,\n",
    "        returning the metric for the generations as in {\"metric_name\": result}.\n",
    "        We encourage to directly load the metric from `evaluate` library to keep the code concise.\n",
    "        :param generations: list(list(str))\n",
    "            list of lists containing generations\n",
    "        :param references: list(str)\n",
    "            list of str containing refrences\n",
    "        :return: dict[str: float]\n",
    "        \"\"\"\n",
    "        return self.metric(generations, references, self.ERROR_TOKEN)\n",
    "\n",
    "    @cache\n",
    "    def fewshot_examples(self):\n",
    "        \"\"\"\n",
    "        Returns a few-shot example for the task.\n",
    "        :param n: int\n",
    "            number of examples\n",
    "        :param seed: int\n",
    "            seed for random number generator\n",
    "        :return: str\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        for doc in self._train.select(range(self._nshot)):\n",
    "            examples.append(self.format_train_example(doc))\n",
    "        return \"\\n\".join(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5f9c6b-bc6a-4650-99d7-b210d357e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FOLIO: Natural Language Reasoning with First-Order Logic\n",
    "https://arxiv.org/pdf/2209.00840.pdf\n",
    "\"\"\"\n",
    "# from eval.base import OWAFOLTask\n",
    "# from eval.tasks.utils import evaluate, convert_to_nltk_rep\n",
    "\n",
    "_CITATION = \"\"\"\n",
    "@article{han2022folio,\n",
    "  title={Folio: Natural language reasoning with first-order logic},\n",
    "  author={Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Benson, Luke and Sun, Lucy and Zubova, Ekaterina and Qiao, Yujie and Burtell, Matthew and others},\n",
    "  journal={arXiv preprint arXiv:2209.00840},\n",
    "  year={2022}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_all_tasks():\n",
    "    def create_task(mode, n):\n",
    "        class FOLIO(FOLIOBase):\n",
    "            def __init__(self):\n",
    "                super().__init__(mode, n)\n",
    "\n",
    "        return FOLIO\n",
    "\n",
    "    return {\n",
    "        f\"folio-{mode}-{n}shot\": create_task(mode, n)\n",
    "        for mode in [\"baseline\", \"scratchpad\", \"neurosymbolic\", \"cot\"]\n",
    "        for n in [1, 2, 4, 8, 16]\n",
    "    }\n",
    "\n",
    "\n",
    "class FOLIOBase(OWAFOLTask):\n",
    "    DATASET_PATH = \"benlipkin/folio\"\n",
    "    DATASET_NAME = None\n",
    "\n",
    "    def __init__(self, mode, n, seed=7):\n",
    "        super().__init__(mode, n)\n",
    "        # process validation dataset\n",
    "        self._dataset = self.reformat_fol_samples(self.dataset[\"validation\"]).shuffle(seed)\n",
    "        self._test = self._dataset.select(range(0, len(self._dataset)))\n",
    "\n",
    "    def reformat_fol_samples(self, dataset):\n",
    "        def reformat_fol_sample(sample):\n",
    "            sample[\"premises-FOL\"] = [\n",
    "                convert_to_nltk_rep(premise) for premise in sample[\"premises-FOL\"]\n",
    "            ]\n",
    "            sample[\"conclusion-FOL\"] = convert_to_nltk_rep(sample[\"conclusion-FOL\"])\n",
    "            try:\n",
    "                assert len(sample[\"premises\"]) == len(sample[\"premises-FOL\"])\n",
    "                label = evaluate(sample[\"premises-FOL\"], sample[\"conclusion-FOL\"])\n",
    "                \n",
    "                assert sample[\"label\"] == label\n",
    "                print(label)\n",
    "            except Exception as e:\n",
    "                # print(f\"Error in parsing FOL: {e}\")\n",
    "                # print(sample)\n",
    "                sample[\"label\"] = self.ERROR_TOKEN\n",
    "            return sample\n",
    "\n",
    "        return dataset.map(reformat_fol_sample).filter(\n",
    "            lambda x: x[\"label\"] != self.ERROR_TOKEN\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30583f9-57ee-4c8a-82b4-fb90cb3f23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProofWriterBase(OWAFOLTask):\n",
    "    DATASET_PATH = \"theoxo/proofwriter-deduction-balanced\"\n",
    "    DATASET_NAME = None\n",
    "\n",
    "    def __init__(self, mode, n, seed=7):\n",
    "        super().__init__(mode, n)\n",
    "        self._test = self.reformat(self.dataset[\"test\"]).shuffle(seed)\n",
    "\n",
    "\n",
    "    def reformat(self, dataset):\n",
    "\n",
    "        def punctuate(s):\n",
    "            if s[-1] not in [\".\", \"?\", \"!\"]:\n",
    "                s += \".\"\n",
    "            return s\n",
    "\n",
    "        def reformat_sample(sample):\n",
    "            sample[\"premises\"] = [punctuate(p) for p in sample.pop(\"theory\").split(\". \")]\n",
    "            sample[\"conclusion\"] = punctuate(sample.pop(\"question\"))\n",
    "            sample[\"label\"] = sample.pop(\"answer\")\n",
    "            return sample\n",
    "\n",
    "        return dataset.map(reformat_sample)\n",
    "\n",
    "class FOLIOBase(OWAFOLTask):\n",
    "    DATASET_PATH = \"benlipkin/folio\"\n",
    "    DATASET_NAME = None\n",
    "\n",
    "    def __init__(self, mode, n, seed=7):\n",
    "        super().__init__(mode, n)\n",
    "        # process validation dataset\n",
    "        self._dataset = self.reformat_fol_samples(self.dataset[\"validation\"]).shuffle(seed)\n",
    "        self._test = self._dataset.select(range(0, len(self._dataset)))\n",
    "\n",
    "    def reformat_fol_samples(self, dataset):\n",
    "        def reformat_fol_sample(sample):\n",
    "            sample[\"premises-FOL\"] = [\n",
    "                convert_to_nltk_rep(premise) for premise in sample[\"premises-FOL\"]\n",
    "            ]\n",
    "            sample[\"conclusion-FOL\"] = convert_to_nltk_rep(sample[\"conclusion-FOL\"])\n",
    "            try:\n",
    "                assert len(sample[\"premises\"]) == len(sample[\"premises-FOL\"])\n",
    "                label = evaluate(sample[\"premises-FOL\"], sample[\"conclusion-FOL\"])\n",
    "                assert sample[\"label\"] == label\n",
    "            except Exception as e:\n",
    "                # print(f\"Error in parsing FOL: {e}\")\n",
    "                # print(sample)\n",
    "                sample[\"label\"] = self.ERROR_TOKEN\n",
    "            return sample\n",
    "\n",
    "        return dataset.map(reformat_fol_sample)\n",
    "        # .filter(\n",
    "        #     lambda x: x[\"label\"] != self.ERROR_TOKEN\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0acc63-9589-4faf-9b94-d8925f86c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folio_create_all_tasks():\n",
    "    def create_task(mode, n):\n",
    "        class FOLIO(FOLIOBase):\n",
    "            def __init__(self):\n",
    "                super().__init__(mode, n)\n",
    "\n",
    "        return FOLIO\n",
    "\n",
    "    return {\n",
    "        f\"folio-{mode}-{n}shot\": create_task(mode, n)\n",
    "        for mode in [\"baseline\", \"scratchpad\", \"neurosymbolic\", \"cot\"]\n",
    "        for n in [1, 2, 4, 8, 16]\n",
    "    }\n",
    "\n",
    "def proofwriter_create_all_tasks():\n",
    "    def create_task(mode, n):\n",
    "        class ProofWriter(ProofWriterBase):\n",
    "            def __init__(self):\n",
    "                super().__init__(mode, n)\n",
    "\n",
    "        return ProofWriter\n",
    "\n",
    "    return {\n",
    "        f\"proofwriter-{mode}-{n}shot\": create_task(mode, n)\n",
    "        for mode in [\"baseline\", \"scratchpad\", \"neurosymbolic\", \"cot\"]\n",
    "        for n in [1, 2, 4, 8, 16]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d99ec946-8eea-4fef-a165-8ba3a0020ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TASK_REGISTRY = {\n",
    "    **folio_create_all_tasks(),\n",
    "    **proofwriter_create_all_tasks(),\n",
    "}\n",
    "\n",
    "ALL_TASKS = sorted(list(TASK_REGISTRY))\n",
    "\n",
    "\n",
    "def get_task(task_name):\n",
    "    try:\n",
    "        return TASK_REGISTRY[task_name]()\n",
    "    except KeyError:\n",
    "        print(\"Available tasks:\")\n",
    "        pprint(TASK_REGISTRY)\n",
    "        raise KeyError(f\"Missing task {task_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898b83d9-f8e3-49bb-aff7-5168355a24e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from pathlib import Path\\nimport os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from pathlib import Path\\nimport os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002658843994140625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 63,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 204,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23277fd5edd4b58be5653784561d039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TASK_REGISTRY\n",
    "folio_neurosymbolic_1shot_task = get_task('folio-neurosymbolic-1shot')\n",
    "folio_neurosymbolic_1shot_dataset = folio_neurosymbolic_1shot_task.get_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4840fd5-d60c-4474-93e1-2c5a58571c07",
   "metadata": {},
   "source": [
    "# Construct New Premises and FOLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a52713d2-b96e-4118-abeb-f610be406036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['A La Liga soccer team ranks higher than another if it receives more points.', 'If two La Liga soccer teams recieve the same points, the team which recieves more points from the games between the two teams ranks higher.', 'Real Madrid and Barcelona are both La Liga soccer teams.', 'In La Liga 2021-2022, Real Madrid recieves 86 points and Barcelon recieves 73 points.', 'In La Liga 2021-2022, Real Madrid and Barcelona both recieve 3 points from the games between them.'] \n",
      "conclusion: In La Liga 2021-2022, Real Madrid ranks higher than Barcelona. \n",
      "chain of thought: Let's think step by step. We want to evaluate if in La Liga 2021-2022, Real Madrid ranks higher than Barcelona. From premise 1, we know that a La Liga soccer team ranks higher than another if it receives more points. From premise 4, we know that in La Liga 2021-2022, Real Madrid received more points than Barcelona. Therefore, in La Liga 2021-2022, Real Madrid received more points than Barcelona, so Real Madrid ranks higher than Barcelona, so the statement is true.\n",
      "ANSWER:\tTrue\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. all y. (LaLiga(x) & LaLiga(y) & MorePoints(x, y) -> HigherRank(x, y))', 'all x. all y. (LaLiga(x) & LaLiga(y) & -MorePoints(x, y) & -MorePoints(y, x) & MorePointsInGameBetween(x, y) -> HigherRank(x, y))', 'LaLiga(RealMadrid) & LaLiga(Barcelona)', 'MorePoints(RealMadrid, Barcelona)', '-MorePointsInGameBetween(RealMadrid, Barcelona) & -MorePointsInGameBetween(Barcelona, RealMadrid)']\n",
      "conclusion-FOL: HigherRank(RealMadrid, Barcelona)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "60\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['All athletes are good at sports.', 'All Olympic gold medal winners are good athletes.', 'No scientists are good at sports.', 'All Nobel laureates are scientists.', 'Amy is good at sports or Amy is an Olympic gold medal winner.', 'If Amy is not a Nobel laureate, then Amy is not an Olympic gold medal winner.'] \n",
      "conclusion: If Amy is not an Olympic gold medal winner, then Amy is a Nobel laureate. \n",
      "chain of thought: Let's think step by step. We want to evaluate the statement \"if Amy is not an Olympic gold medal winner, then Amy is a Nobel laureate\". Let's assume that Amy is not an Olympic gold medal winner. This doesn't tell us anything about whether Amy is a Nobel laureate, so the statement isn't true, meaning it is either False or Uncertain. To distinguish between the two, notice that we could have a scenario where Amy is neither an Olympic gold medal winner nor a Nobel laureate. None of the premises are violated in this case. This means the statement must be false.\n",
      "ANSWER:\tFalse\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Athlete(x) -> GoodAtSports(x))', 'all x. (OlympicGoldMedalWinner(x) -> Athlete(x))', 'all x. (Scientist(x) -> -GoodAtSports(x))', 'all x. (NobelLaureate(x) -> Scientist(x))', 'GoodAtSports(Amy) | OlympicGoldMedalWinner(Amy)', '-NobelLaureate(Amy) -> -OlympicGoldMedalWinner(Amy)']\n",
      "conclusion-FOL: -OlympicGoldMedalWinner(Amy) -> NobelLaureate(Amy)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "125\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['All dispensable things are environment-friendly. ', 'All woodware is dispensable.', 'All paper is woodware. ', 'No good things are bad. ', 'All environment-friendly things are good.', 'A worksheet is either paper or is environment-friendly.'] \n",
      "conclusion: A worksheet is not dispensable. \n",
      "chain of thought: Let's think step by step. We want to evaluate if a worksheet is not dispensable. From premise 6, we know that a worksheet is either paper or is environment-friendly. If it is paper, then from premise 3, a worksheet is woodware, and from premise 2, a worksheet is dispensable. If it is environment-friendly, we know it is good from premise 5, but we know nothing about whether it is dispensable. Therefore, we don't know if a worksheet is dispensible or not, so the statement is uncertain.\n",
      "ANSWER:\tUncertain\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Dispensable(x) -> EnvironmentFriendly(x))', 'all x. (Woodware(x) -> Dispensable(x))', 'all x. (Paper(x) -> Woodware(x))', 'all x. (Good(x) -> -Bad(x))', 'all x. (EnvironmentFriendly(x) -> Good(x))', '((Paper(Worksheet) & -EnvironmentFriendly(Worksheet)) | (-Paper(Worksheet) & EnvironmentFriendly(Worksheet)))']\n",
      "conclusion-FOL: -Dispensable(Worksheet)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "148\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['No songs are visual. ', 'All folk songs are songs. ', 'All videos are visual. ', 'All movies are videos.', 'All sci-fi movies are movies.', 'Inception is a sci-fi movie.', 'Mac is neither a folk song nor a sci-fi movie.'] \n",
      "conclusion: Inception is a folk song. \n",
      "chain of thought: Let's think step by step. We want to evaluate if Inception is a folk song. We know that Inception is a sci-fi movie. Since all movies are videos and Inception is a movie, it is a video, which means it is visual. On the other hand, we know that all folk songs are songs, and no songs are visual, so no folk songs are visual. Therefore, since Inception is visual but no folk songs are visual, we know that Inception cannot be a folk song, so the statement is false.\n",
      "ANSWER:\tFalse\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Song(x) -> -Visual(x))', 'all x. (FolkSong(x) -> Song(x))', 'all x. (Video(x) -> Visual(x))', 'all x. (Movie(x) -> Video(x))', 'all x. (ScifiMovie(x) -> Movie(x))', 'ScifiMovie(Inception)', '-FolkSong(Mac) & -ScifiMovie(Mac)']\n",
      "conclusion-FOL: FolkSong(Inception)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "261\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['Every chef can cook.', 'Some people who aren’t chefs can cook.', 'People who cook can make scrambled eggs and pasta.', 'If someone can make cookies and muffins, they are a baker.', 'Bakers who can also make scrambled eggs can make a good breakfast.', 'Luke can make cookies, scrambled eggs, and muffins, but not pasta.'] \n",
      "conclusion: Luke can make a good breakfast. \n",
      "chain of thought: Let's think step by step. We want to evaluate if Luke can make a good breakfast. From the last premise, we know that Luke can make cookies, scrambled eggs, and muffins. Since Luke can make cookies and muffins, they are a baker. Now, combining the information we have, since Luke is a baker and can make scrambled eggs, this means that they can make a good breakfast. Therefore, Luke can make a good breakfast, so the statement is true.\n",
      "ANSWER:\tTrue\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Chef(x) -> Cook(x))', 'exists x. (-Chef(x) & Cook(x))', 'all x. (Cook(x) -> (MakeScrambledEggs(x) & MakePasta(x)))', 'all x. (MakeCookies(x) & MakeMuffins(x) -> Baker(x))', 'all x. ((Baker(x) & MakeScrambledEggs(x)) -> MakeGoodBreakfast(x))', 'MakeCookies(Luke) & MakeScrambledEggs(Luke) & MakeMuffins(Luke) & -MakePasta(Luke)']\n",
      "conclusion-FOL: MakeGoodBreakfast(Luke)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "263\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['ETS develops various standardized tests primarily in the United States for K-12 and higher education. ', 'ETS administers international tests including the TOEFL, TOEIC, GRE and subject tests in more than 180 countries.', 'Many of the assessments ETS develops are associated with entry to the US tertiary and quaternary education institutions. ', 'ETS also develops K-12 statewide assessments used for accountability testing in many states.'] \n",
      "conclusion: ETS develops assessments for K-12 statewide as well as entry to US tertiary and quaternary education institutions. \n",
      "chain of thought: Let's think step by step. We want to evaluate if ETS develops assessments for K-12 statewide as well as entry to US tertiary and quaternary educatiand doon institutions. We know that ETS develops assessments for K-12 statewide. We also know that ETS develops assessments associated with entry to the US tertiary and quaternary education institutes. Therefore, both parts of the conclusion are true, and the statement is true.\n",
      "ANSWER:\tTrue\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['exists x. (Develops(Ets, x) & StandardizedTest(x) & In(x, Unitedstates) & For(x, k-OneTwoandhighereducation))', 'exists x. (Administers(Ets, x) & InternationalTest(x) & (TOEFL(x) | TOEIC(x) | GRE(x) | SubjectTests(x)))', 'exists x. (Develops(Ets, x) & AssociatedWith(x, Entrytouseducationinstitutions))', 'exists x. (Develops(Ets, x) & StateWideAssesment(x) & UsedFor(x, Accountabilitytesting))']\n",
      "conclusion-FOL: exists x. (Develops(Ets, x) & For(x, k-OneTwoandhighereducation)) & exists x. (Develops(Ets, x) & AssociatedWith(x, Entrytouseducationinstitutions))\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "275\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['All people who are respected by others are people who contribute to the country. ', 'If a person is respected by others, then he/she contributes to the country. ', 'All people available to have a visit without any fees are those respected by others. ', 'All Customers who once served in the army are available to have a visit without any fees. ', 'All people who once were sentenced for thief stayed in prison for some time. ', 'All people who once stayed in prison for some time have a bad record in the local state. ', 'James was either once sentenced for thief or stayed in prison for some time. ', 'James is either with a bad record in the local state or respected by others. '] \n",
      "conclusion: James contributes to the country. \n",
      "chain of thought: Let's think step by step. We want to evaluate if James contributes to the country. Let's think about what we know about James. First, we know that James was either sentenced for thief or stayed in prison. However, this doesn't tell us anything about whether James contributed to the country. Second, we know that James either had a bad record in the local state or that he was respected by others. However, the premises don't tell us anything about the relationship between having a bad record and contributing to the country. Therefore, it is uncertain whether James contributes to the country.\n",
      "ANSWER:\tUncertain\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Respected(x) -> ContributeToCountry(x))', 'all x. (Respected(x) -> ContributeToCountry(x))', 'all x. (HaveVisitWithoutAnyFees(x) -> Respected(x))', 'all x. (Army(x) -> HaveVisitWithoutAnyFees(x))', 'all x. (Thief(x) -> Prison(x))', 'all x. (Prison(x) -> BadRecord(x))', '((Thief(James) & -Prison(James)) | (-Thief(James) & Prison(James))) ', '((BadRecord(James) & -Respected(James)) | (-BadRecord(James) & Respected(James)))']\n",
      "conclusion-FOL: ContributeToCountry(James)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "299\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['All students learning piano can strike the right notes. ', 'All students who can strike the right note can get the rhythms right. ', 'If a student can get the rhythms right, he will start working on coordination between the left and the right hands. ', 'Some students who start working on coordination between the left and the right hands become good at it. The other students find it challenging. ', 'If John can strike the right notes, get the rhythms right and is good at coordination between right and left hands, then he puts emotions into his playing. ', 'John is a student learning piano. ', 'John does not find coordination between the left and the right hands challenging. '] \n",
      "conclusion: John can get the rhythms right. \n",
      "chain of thought: Let's think step by step. We want to evaluate if John can get the rhythms right. We know that John is a student learning piano. Since all students learning piano can strike the right notes, John can strike the right notes. Since all students who can strike the right notes can get the rhythms right and John can strike the right notes, John can get the rhythms right, so the conclusion is true.\n",
      "ANSWER:\tTrue\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Student(x) & LearningPiano(x) -> StrikeRightNotes(x))', 'all x. (Student(x) & StrikeRightNotes(x) -> GetRhythmRight(x))', 'all x. (Student(x) & GetRhythmRight(x) -> StartCoordination(x))', 'all x. (Student(x) & StartCoordination(x) -> ((GoodAtCoordination(x) & -FindCoordinationChallenging(x)) | (-GoodAtCoordination(x) & FindCoordinationChallenging(x))))', 'StrikeRightNotes(John) & GetRhythmRight(John) & GoodAtCoordination(John) -> PutEmotion(John)', 'Student(John) & LearningPiano(John)', '-FindCoordinationChallenging(John)']\n",
      "conclusion-FOL: GetRhythmRight(John)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "683\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['China is one of BRICS and its economy is emerging.', 'India is one of BRICS and its economy is emerging.', 'All people from China speak Chinese.', 'All people from India speak Hindi or English.', 'There is an Indian.'] \n",
      "conclusion: There is a person from BRICS speaking Hindi. \n",
      "chain of thought: Let's think step by step. We want to evaluate if there is a person from BRICS speaking Hindi. We know that there is an Indian, and since India is one of BRICS, we know that there is an Indian in BRICS. Furthermore, we know that they speak either Hindi or English, however, we don't know which one. Therefore, there could be a person in BRICS speaking Hindi, or there could not. Therefore, it is uncertain whether there is a person from BRICS speaking Hindi.\n",
      "ANSWER:\tUncertain\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (China(x) -> BRICS(x) & EmergingEconomy(x))', 'all x. (India(x) -> BRICS(x) & EmergingEconomy(x))', 'all x. all y. (From(x, y) & China(y) -> Speak(x, Chinese))', 'all x. all y. (From(x, y) & India(y) -> Speak(x, Hindi) | Speak(x, English))', 'exists x. exists y. (From(x, y) & India(y))']\n",
      "conclusion-FOL: exists x. (BRICS(x) & Speaks(x, Hindi))\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "684\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['Daveed Diggs is an actor and film producer.', 'Daveed Diggs played two roles in the musical Hamilton.', 'One of the actors from Hamilton won the best actor award.', 'The actor playing Thomas Jefferson won the best actor award.', 'Daveed Diggs played Thomas Jefferson.', 'Musicals are not films.'] \n",
      "conclusion: Hamilton is a film. \n",
      "chain of thought: Let's think step by step. We want to evaluate if Hamilton is a film. Since Daveed Diggs played two roles in the musical Hamilton, Hamilton is a musical. Since musicals are not films and Hamilton is a musical, Hamilton is not a film, and the conclusion is false.\n",
      "ANSWER:\tFalse\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['Actor(DaveedDiggs) & FilmProducer(DaveedDiggs)', 'PlayedTwoRoles(DaveedDiggs) & PlayedIn(DaveedDiggs, Hamilton) & Musical(Hamilton)', 'exists x. (Actor(x) & PlayedIn(x, Hamilton) & WonBestActorAward(x))', 'all x. (Actor(x) & Played(x, ThomasJefferson) -> WonBestActorAward(x))', 'Played(DaveedDiggs, ThomasJefferson)', 'all x. (Musical(x) -> -Film(x))']\n",
      "conclusion-FOL: Film(Hamilton)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "850\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['All pets are animals.', 'Pets can be either a dog or a cat.', 'If a person has a pet, they care for that pet. ', 'Dogs and Cats can be naughty. ', 'Pets who are naughty are not liked as much. ', 'Charlie has a naughty pet dog named Leo. '] \n",
      "conclusion: Charlie does not like Leo and does not care for Leo. \n",
      "chain of thought: Let's think step by step. We want to evaluate if Charlie does not like Leo and does not care for Leo. Let's first evaluate if Charlie does not like Leo. We know Charlie has a naughty pet named Leo. Since pets who are naughty are not liked as much, Charlie does not like Leo. Now, let's evaluate if Charlie cares for Leo. We know that if a person has a pet, they care for that pet. Since Leo is Charlie's pet, Charlie cares for Leo. Therefore, Charlie does not like Leo but cares for Leo, so the second part of the conclusion is false, which means the entire conclusion is false.\n",
      "ANSWER:\tFalse\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Pet(x) -> Animal(x))', 'all x. (Pet(x) -> ((Dog(x) & -Cat(x)) | (-Dog(x) & Cat(x))))', 'all x. all y. (HasPet(x) -> Cares(x, y))', 'exists x. ((Cat(x) & Naughty(x)) | (Dog(x) & Naughty(x)))', 'all x. all y. (Pet(x) & Naughty(x) -> -Liked(x, y))', 'HasPet(Charlie) & Pet(Leo) & Dog(Leo) & Naughty(Leo)']\n",
      "conclusion-FOL: -Liked(Leo, Charlie) & -Cares(Charlie, Leo)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "853\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['All books written by Cixin Liu have sold more than 1 million copies. ', 'Some books that have won the Hugo Award were written by Cixin Liu.', 'All books about the future are influenced by Isaac Asimov.', 'The book the Three Body Problem has sold more than 1 million copies.', 'The Three Body Problem is about the future.'] \n",
      "conclusion: The Three Body Problem won the Hugo Award. \n",
      "chain of thought: Let's think step by step. We want to evaluate if the Three Body Problem won the Hugo Award. The only thing we know about the Hugo Award is that some books that have won the Hugo Award were written by Cixin Liu. However, we know nothing about whether The Three Body Problem was written by Cixin Liu, so the conclusion is uncertain.\n",
      "ANSWER:\tUncertain\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Book(x) & WrittenBy(x, Cixinliu) -> SoldMoreThan(x, Onemillion))', 'exists x. (Won(x, Hugoaward) & Book(x) & WrittenBy(x, Cixinliu))', 'all x. (Book(x) & AboutFuture(x) -> InfluencedBy(x, Isaacasimov))', 'Book(Threebodyproblem) & SoldMoreThan(Threebodyproblem, Onemillion)', 'AboutFuture(Threebodyproblem)']\n",
      "conclusion-FOL: Won(Threebodyproblem, Hugoaward)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "886\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['Dagfinn is a given name.', 'Notable people with the given name include Dagfinn Aarskog, Dagfinn Bakke and Dagfinn Dahl. ', 'Dagfinn Aarskog is a Norwegian physician.', 'Dagfinn Dahl is a Norwegian barrister.'] \n",
      "conclusion: Dagfinn is Dagfinn Aarskog's given name. \n",
      "chain of thought: Let's think step by step. We want to evaluate if Dagfinn is Dagfinn Aarskog's given name. We know that Dagfinn is a given name, and that notable people with the given name Dagfinn includes Dagfinn Aarskog, which means that Dagfinn is Dagfinn Aarskog's given name, so the conclusion is true.\n",
      "ANSWER:\tTrue\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. (Dagfinn(x) -> GivenName(x))', 'Dagfinn(DagfinnAarskog) & NotablePeople(DagfinnAarskog) & Dagfinn(DagfinnBakke) & NotablePeople(DagfinnBakke) & Dagfinn(DagfinnDahl) & NotablePeople(DagfinnDahl)', 'Norwegian(DagfinnAarskog) & Physician(DagfinnAarskog)', 'Norwegian(DagfinnDahl) & Barrister(DagfinnDahl)']\n",
      "conclusion-FOL: Dagfinn(DagfinnAarskog)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "892\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['St Johnstone is a Scottish team.', 'St Johnstone is part of the Scottish Premiership.', 'If a team is part of the league, it has joined the league.', 'St Johnstone and Minsk are different teams.', 'For two different teams, either one team wins or the other team wins.', 'Minsk won against St Johnstone.'] \n",
      "conclusion: Minsk joined the Scottish Premiership. \n",
      "chain of thought: Let's think step by step. We want to evaluate if Minsk joined the Scottish Premiership. We know that Minsk and St Johnstone are different teams and that St Johnstone is part of the Scottish Premiership, but we don't know anything about whether or not Minsk joined the Scottish Premiership from the premises. Therefore, the conclusion is uncertain.\n",
      "ANSWER:\tUncertain\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['Scottish(Johnstone)', 'PartOf(Johnstone, Scottishpremiership)', 'all x. all y. (PartOf(x, y) -> Joined(x, y))', 'Different(Misnk, Johnstone) & Different(Johnstone, Minsk)', 'all x. all y. (Different(x, y) -> ((WonGame(x, y) & -WonGame(y, x)) | (-WonGame(x, y) & WonGame(y, x))))', 'WonGame(Minsk, Johnstone)']\n",
      "conclusion-FOL: PartOf(Minsk, Scottishpremiership)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "930\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['Boves is a railway station located in France. ', 'The preceding station of Boves is Longueau.', 'The preceding station of Dommartin is Boves.', 'France is a European country.', 'Dommartin is situated on the Paris–Lille railway. ', 'Any two contiguous stations are on the same railway.', 'Boves is served by regional TER Hauts-de-France trains.', 'If A is located in B and B is located in C, then A is located in C.', 'If A precedes B and B preceds C, than A preceds C.'] \n",
      "conclusion: Boves is not in Europe. \n",
      "chain of thought: Let's think step by step. We want to evaluate if Boves is not in Europe. We know that Boves is a railway station located in France. We also know that since France is a European country, France is located in Europe. Furthermore, we know that if A is located in B and B is located in C, then A is located in C. Therefore, we know that because Boves is located in France and France is located in Europe, that means Boves is located in Europe. Therefore, the conclusion is false.\n",
      "ANSWER:\tFalse\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['RailwayStation(Boves) & Locate(Boves, France)', 'Precede(Boves, Longueau)', 'Precede(Dommartin, Boves)', 'Locate(France, Europe)', 'Situate(Dommartin, PairsLille)', 'all x. all y. all z. ((Situate(x, z) & (Precede(x, y) | Precede(y, x))) -> Situate(y, z))', 'Serve(HautsDeFrance, Boves)', 'all x. all y. all z. ((Locate(x, y) & Locate(y, z)) -> Locate(x, z))', 'all x. all y. all z. ((Precede(x, y) & Precede(y, z)) -> Precede(x, z))']\n",
      "conclusion-FOL: -Locate(Boves, Europe)\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n",
      "980\n",
      "Given the following premises, conclusion and correspoding chain of thought:\n",
      "premises: ['Either present their work at the conference or provide a tutorial session at the conference. ', 'All who present their work at the conference will attend in person. ', 'All those providing a tutorial session at the conference are invited to join the club. ', 'All who attend the conference in person are provided with souvenirs. ', 'All invited to join the club are provided with delicious meals. ', 'All provided with delicious meals are happy to communicate with each other during the dinner. ', 'All provided with delicious meals are invited to take a photo with the audience. ', 'James does not attend the conference in person and is not provided with souvenirs.'] \n",
      "conclusion: James is either invited to take a photo with the audience or happy to communicate with each other during the dinner. \n",
      "chain of thought: Let's think step by step. We want to evaluate if James is either invited to take a photo with the audience or happy to communicate with each other during the dinner. We know that James does not attend the conference in person and is not provided with souvenirs. There are no premises that apply to people who do not attend the conference. Since James is not provided with souvenirs, since all who attended the conference in person are provided with souvenirs, we know that James did not attend the conference in person. However, we don't know anything else, so it is possible that James was neither invited to take a photo with the audience nor happy to communicate during the dinner. Therefore, the conclusion is false.\n",
      "ANSWER:\tFalse\n",
      "Moreover, FOL expressions of these premises and conclusion are as the following: \n",
      "premises-FOL: ['all x. ( Present(x) | Tutorial(x))', 'all x. (Present(x) -> InPerson(x))', 'all x. (Tutorial(x) -> Invited(x))', 'all x. (InPerson(x) -> Souvenirs(x))', 'all x. (Invited(x) -> Meals(x))', 'all x. (Meals(x) -> HappyCommunicate(x))', 'all x. (Meals(x) -> InvitedTakePhoto(x))', '-(InPerson(James) & Souvenirs(James))']\n",
      "conclusion-FOL: (InvitedTakePhoto(James) & -HappyCommunicate(James)) | (-InvitedTakePhoto(James) & HappyCommunicate(James))\n",
      "please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# indexs_list = [\n",
    "#             125,\n",
    "#             23,\n",
    "#             60,\n",
    "#             275,\n",
    "#             148,\n",
    "#             261,\n",
    "#             263,\n",
    "#             683,\n",
    "#             299,\n",
    "#             684,\n",
    "#             850,\n",
    "#             853,\n",
    "#             886,\n",
    "#             892,\n",
    "#             930,\n",
    "#             980,\n",
    "#         ]\n",
    "\n",
    "indexs_list = [\n",
    "    23,\n",
    "    60, \n",
    "    125, \n",
    "    148,\n",
    "    261, \n",
    "    263, \n",
    "    275, \n",
    "    299, \n",
    "    683, \n",
    "    684, \n",
    "    850, \n",
    "    853, \n",
    "    886, \n",
    "    892, \n",
    "    930, \n",
    "    980 \n",
    "]\n",
    "\n",
    "for index in indexs_list:\n",
    "    td = folio_neurosymbolic_1shot_task._train_dataset[index]\n",
    "    print(index)\n",
    "    print(\"Given the following premises, conclusion and correspoding chain of thought:\")\n",
    "    print(\"premises: {} \\nconclusion: {} \\nchain of thought: {}\".format(\n",
    "        td['premises'], \n",
    "        td['conclusion'], \n",
    "        td['cot']))\n",
    "    print(\"Moreover, FOL expressions of these premises and conclusion are as the following: \")\n",
    "    print(\"premises-FOL: {}\\nconclusion-FOL: {}\".format(\n",
    "        td['premises-FOL'], \n",
    "        td['conclusion-FOL']))\n",
    "    print(\"please extract new premises from the chain of thought and translate them into FOL forms according the above FOL examples, and give new premises and corresponding FOLs in python list format:\")\n",
    "    print('\\n \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a258a78-df62-4205-bfe6-33e37f8fa5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50322ef5-d012-4ae5-853e-b1808dde2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 125\n",
    "new_premises = [\n",
    "    \"If a worksheet is paper, then it is woodware\",\n",
    "    \"If a worksheet is woodware, then it is dispensable\",\n",
    "    \"If a worksheet is environment-friendly, it is good\"\n",
    "]\n",
    "\n",
    "new_premises_FOL = [\n",
    "    \"Paper(Worksheet) -> Woodware(Worksheet)\",\n",
    "    \"Woodware(Worksheet) -> Dispensable(Worksheet)\",\n",
    "    \"EnvironmentFriendly(Worksheet) -> Good(Worksheet)\"\n",
    "]\n",
    "\n",
    "# index 23\n",
    "# New Premises\n",
    "new_premises = [\n",
    "    \"Real Madrid received more points than Barcelona in La Liga 2021-2022\",\n",
    "    \"A team that receives more points ranks higher than the other in La Liga\"\n",
    "]\n",
    "\n",
    "# FOL Translations\n",
    "new_premises_FOL = [\n",
    "    \"MorePoints(RealMadrid, Barcelona)\",\n",
    "    \"all x. all y. (LaLiga(x) & LaLiga(y) & MorePoints(x, y) -> HigherRank(x, y))\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac62f2-b252-4179-bcc5-29f7cead296d",
   "metadata": {},
   "source": [
    "# Perform Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec8825-cc26-445f-8aa6-36f4ac1186e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_premises_from_cots = {\n",
    "            23: [\n",
    "    \"Real Madrid received more points than Barcelona in La Liga 2021-2022\",\n",
    "    \"A team that receives more points ranks higher than the other in La Liga\"\n",
    "]\n",
    "            60: \n",
    "            125: \n",
    "            148: \n",
    "            261: \n",
    "            263: \n",
    "            275: \n",
    "            299: \n",
    "            683: \n",
    "            684: [\n",
    "    \"Hamilton is a musical\",\n",
    "    \"Musicals are not films\"\n",
    "],\n",
    "            850: [\n",
    "    \"Charlie has a naughty pet named Leo, and Leo is a dog\",\n",
    "    \"Charlie does not like Leo\",\n",
    "    \"Charlie cares for Leo\"\n",
    "],\n",
    "            853: [\n",
    "    \"The relationship between The Three Body Problem and Cixin Liu is not established\",\n",
    "    # \"The Three Body Problem has sold more than 1 million copies and is about the future\"\n",
    "],\n",
    "            886: [\n",
    "    \"Dagfinn is a given name\",\n",
    "    \"Dagfinn Aarskog is a notable person with the given name Dagfinn\"\n",
    "],\n",
    "\n",
    "            892: [\n",
    "    \"St Johnstone and Minsk are different teams\",\n",
    "    \"Minsk won against St Johnstone\",\n",
    "    # \"There is no information about Minsk being part of the Scottish Premiership\"\n",
    "],\n",
    "            930: [\n",
    "    \"Boves is located in France\",\n",
    "    \"France is located in Europe\",\n",
    "    \"If a location is in another location, which in turn is in a third location, then the first location is also in the third location\"\n",
    "],\n",
    "            980: [\n",
    "    \"James did not attend the conference in person\",\n",
    "    \"James was not provided with souvenirs\",\n",
    "    # \"There is no direct information about whether James was invited to join the club or provided with delicious meals\"\n",
    "]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06560902-c93d-4593-ae50-bb773e927fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_premises_fols_from_cots = {\n",
    "            23: [\n",
    "    \"MorePoints(RealMadrid, Barcelona)\",\n",
    "    \"all x. all y. (LaLiga(x) & LaLiga(y) & MorePoints(x, y) -> HigherRank(x, y))\"\n",
    "]\n",
    "            60: \n",
    "            125: \n",
    "            148: \n",
    "            261: \n",
    "            263: \n",
    "            275: \n",
    "            299: \n",
    "            683: \n",
    "            684: \n",
    "            850: [\n",
    "    \"HasPet(Charlie) & Pet(Leo) & Dog(Leo) & Naughty(Leo)\",\n",
    "    \"-Liked(Leo, Charlie)\",\n",
    "    \"Cares(Charlie, Leo)\"\n",
    "],\n",
    "            853: [\n",
    "    \"Book(Threebodyproblem) & SoldMoreThan(Threebodyproblem, Onemillion) & AboutFuture(Threebodyproblem)\"\n",
    "    # The first premise is not translated into FOL due to its nature.\n",
    "],\n",
    "            886: [\n",
    "    \"all x. (Dagfinn(x) -> GivenName(x))\",\n",
    "    \"Dagfinn(DagfinnAarskog) & NotablePeople(DagfinnAarskog)\"\n",
    "],\n",
    "            892: [\n",
    "    \"Different(Minsk, Johnstone) & Different(Johnstone, Minsk)\",\n",
    "    \"WonGame(Minsk, Johnstone)\"\n",
    "    # The third premise is not translated into FOL due to its nature.\n",
    "],\n",
    "            930: [\n",
    "    \"Locate(Boves, France)\",\n",
    "    \"Locate(France, Europe)\",\n",
    "    \"all x. all y. all z. ((Locate(x, y) & Locate(y, z)) -> Locate(x, z))\"\n",
    "],\n",
    "            980:  [\n",
    "    \"-InPerson(James)\",\n",
    "    \"-Souvenirs(James)\"\n",
    "    # The third premise is not translated into FOL due to its nature.\n",
    "]\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
