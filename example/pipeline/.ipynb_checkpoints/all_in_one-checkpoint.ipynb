{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff2170e-92a0-4507-be8c-0ea59ada88a3",
   "metadata": {},
   "source": [
    "![](../../docs/colorful_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c32da6-2647-4446-809b-be93d4dcbefe",
   "metadata": {},
   "source": [
    "# Ananke Pipeline Description\n",
    "![](structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b94d807-89ed-4152-a4bf-7b725257e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"]=\"http://192.168.1.248:33210/\"\n",
    "os.environ[\"https_proxy\"]=\"http://192.168.1.248:33210/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47378e94-4ffa-4ad7-8528-4b5ddee8036a",
   "metadata": {},
   "source": [
    "# Structure Design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5f7b1-99df-4c8f-b9f1-a953ad263c64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b52f17b-8d21-4d7e-ae1e-9c9c29d99f75",
   "metadata": {},
   "source": [
    "## Interaction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce44a1e-eb87-472a-aa0a-22e5a88227a8",
   "metadata": {},
   "source": [
    "### UserInterface (No in startup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5e756-120c-4f89-b8c4-c68063bca650",
   "metadata": {},
   "source": [
    "> 在初级的交互式界面中主要使用nodejs + bootstrap\n",
    " \n",
    "\n",
    " - [x] 任务需要跑通koodo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3dc8f-b518-495e-b25a-b5ea0fea8f86",
   "metadata": {},
   "source": [
    "### Client(Local/Web)Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122c83f-c891-45b9-8881-768522292344",
   "metadata": {},
   "source": [
    "服务发送以内部http request or 直接调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd988515-691a-4756-b5df-679419d08426",
   "metadata": {},
   "source": [
    "#### Client API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685360e-7ede-4c06-8e58-532c54e2e4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67271cb-8e17-413c-b353-cfce5541dcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149d9aa-87f1-4806-a0ed-d211923a40c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e645cf-cc8a-4c67-8f88-ff01dc17a8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e75a4f0-08a8-4182-a883-16bbd40e13bd",
   "metadata": {},
   "source": [
    "#### Server API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735ea9b-2aec-4c0b-9b7f-59bf5e3faa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1558527-b17e-4fe6-be38-d7bc4fd547b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e256728-3f78-4983-aec9-1b598e7f793e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5adbe02a-2c6b-499d-875d-13802d05bc04",
   "metadata": {},
   "source": [
    "### LLM Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb870ce-e136-49bf-b875-91d782666876",
   "metadata": {},
   "source": [
    "\n",
    "LLM Support List：\n",
    "- OpenAI\n",
    "- Anthropic\n",
    "- Gradient\n",
    "- Hugging Face\n",
    "- EverlyAI\n",
    "- LiteLLM\n",
    "- PaLM\n",
    "- Predibase\n",
    "- Replicate\n",
    "- LangChain\n",
    "- Llama API\n",
    "- Llama CPP\n",
    "- Xorbits Inference\n",
    "- MonsterAPI\n",
    "- RunGPT\n",
    "- Portkey\n",
    "- AnyScale\n",
    "- Ollama\n",
    "- Konko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b6ace-d512-4acf-8f19-69be132a2d6d",
   "metadata": {},
   "source": [
    "### Context Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669855b0-c86b-47f4-8d6d-75a067b9e02e",
   "metadata": {},
   "source": [
    "主要分为Context构造压缩重复存储&结构化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da654e9e-1e8a-4bf8-b402-b5537b1b4997",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf7e75-5d08-45ee-b087-0b92bf3ede4b",
   "metadata": {},
   "source": [
    "在传统的定义模式中类似langchain & llamaindex 他们的prompt实现忽视了不同阶段的定义例子如下\n",
    "```python\n",
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.prompts import ChatPromptTemplate\n",
    "\n",
    "# Text QA Prompt\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=\"Always answer the question, even if the context isn't helpful.\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=(\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge, \"\n",
    "            \"answer the question: {query_str}\\n\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
    "```\n",
    "这种定义形式忽略了初始prompt和持续对话prompt对定义的不同需求，即初始定义为可选项，在前向对话的时候引入另一组prompt预设。当然随着历史对话的增长输出性能会随之下降\n",
    "大量的情况，在固定context长度的时候这种长对话不建议超过max token\n",
    "\n",
    "**** \n",
    "因此在框架中引入了下列新的构造形式："
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d8b2f78-d37b-4ca6-83ce-023d229f7065",
   "metadata": {},
   "source": [
    "\n",
    "from ananke.prompt import Prompt\n",
    "prompt = Prompt()\n",
    "\n",
    "instructed_init_prompt = \"\"\"\n",
    "The below text is you need to process\n",
    "```markdown\n",
    "{text}\n",
    "{extra_param}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "instructed_forward_prompt = \"\"\"\n",
    "The below data is you need process:\n",
    "```\n",
    "Nodes:\n",
    "```\n",
    "{nodes}\n",
    "```\n",
    "Relationships:\n",
    "```\n",
    "{relationships}\n",
    "```\n",
    "questions or sentences:\n",
    "```\n",
    "{user_input}\n",
    "\"\"\"\n",
    "\n",
    "prompt.set_template(instructed_init_prompt, instructed_forward_prompt)\n",
    "\n",
    "init_formatted_prompt = prompt.init(text=\"Sample text to process\", extra_param=\"Extra parameter\")\n",
    "forward_formatted_prompt = prompt(nodes=\"Node data\", relationships=\"Relationship data\", user_input=\"User input\")\n",
    "\n",
    "print(\"Initialized Prompt:\")\n",
    "print(init_formatted_prompt)\n",
    "\n",
    "print(\"\\nForward Prompt:\")\n",
    "print(forward_formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad16e1c-744b-4dec-8594-997f5d273255",
   "metadata": {},
   "source": [
    "因此我们可以看到这个结构充分的扩展了构造结果的适用性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3955b3-80c6-444b-9bb8-0f3a9ab28f9d",
   "metadata": {},
   "source": [
    "#### Compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7b016-e4a2-4e0d-a3f4-7027d07a6406",
   "metadata": {},
   "source": [
    "这类问题主要源于针对某一问题的context压缩,这一问题的主要来源是我们通常并不能从一个给定的向量数据库查询结果的文章段落中很好的突出重点信息。而和问题相关的内容通常只占相关段落的很少一部分，因此需要对context进行筛选和压缩\n",
    "\n",
    "One challenge with retrieval is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.\n",
    "\n",
    "To use the Contextual Compression Retriever, you'll need:\n",
    "\n",
    "* a base retriever\n",
    "* a Document Compressor\n",
    "The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.\n",
    "\n",
    "![](compressor.jpg)\n",
    "\n",
    "下面是一个langchain的官方案例：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd15ad7-b015-4b4f-9c72-934f453cb1f3",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "documents = TextLoader('../../../state_of_the_union.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"What did the president say about Ketanji Brown Jackson\")\n",
    "pretty_print_docs(docs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f3e4c-28c9-4dab-a412-62a0a94ec05e",
   "metadata": {},
   "source": [
    "```markdown\n",
    "    Document 1:\n",
    "    \n",
    "    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
    "    \n",
    "    Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
    "    \n",
    "    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
    "    \n",
    "    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    Document 2:\n",
    "    \n",
    "    A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n",
    "    \n",
    "    And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n",
    "    \n",
    "    We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n",
    "    \n",
    "    We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n",
    "    \n",
    "    We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n",
    "    \n",
    "    We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    Document 3:\n",
    "    \n",
    "    And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \n",
    "    \n",
    "    As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \n",
    "    \n",
    "    While it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \n",
    "    \n",
    "    And soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \n",
    "    \n",
    "    So tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \n",
    "    \n",
    "    First, beat the opioid epidemic.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    Document 4:\n",
    "    \n",
    "    Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \n",
    "    \n",
    "    And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \n",
    "    \n",
    "    That ends on my watch. \n",
    "    \n",
    "    Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \n",
    "    \n",
    "    We’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \n",
    "    \n",
    "    Let’s pass the Paycheck Fairness Act and paid leave.  \n",
    "    \n",
    "    Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \n",
    "    \n",
    "    Let’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf3a03-c64d-4992-916e-f081256c893f",
   "metadata": {},
   "source": [
    "我们可以看到，给定一个示例问题，我们的检索器返回一两个相关文档和一些不相关的文档。甚至相关文档中也有很多不相关的信息。这对于整个内容的后续交互和生成是极其不必要的，在langchain中内置了很多压缩和过滤的模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af4cc6-fce1-4392-80a5-7ea0d12f5aa6",
   "metadata": {},
   "source": [
    "* LLMChainFilter\n",
    "\n",
    "The LLMChainFilter is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d2d63-258b-4260-9284-4e2746541b43",
   "metadata": {},
   "source": [
    "* EmbeddingsFilter\n",
    "\n",
    "Making an extra LLM call over each retrieved document is expensive and slow. The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75989475-00da-4857-a47e-2219f76280c8",
   "metadata": {},
   "source": [
    "* Stringing compressors and document transformers together\n",
    "\n",
    "Using the DocumentCompressorPipeline we can also easily combine multiple compressors in sequence. Along with compressors we can add BaseDocumentTransformers to our pipeline, which don't perform any contextual compression but simply perform some transformation on a set of documents. For example TextSplitters can be used as document transformers to split documents into smaller pieces, and the EmbeddingsRedundantFilter can be used to filter out redundant documents based on embedding similarity between documents.\n",
    "\n",
    "Below we create a compressor pipeline by first splitting our docs into smaller chunks, then removing redundant documents, and then filtering based on relevance to the query.\n",
    "\n",
    "```python\n",
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb0e70-f425-4066-8ed9-76cd223e3294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c134dc50-7db3-496f-815c-5a3ddb5b3187",
   "metadata": {},
   "source": [
    "#### TempStorage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdffae-5925-4a35-86f6-59db690e2bfd",
   "metadata": {},
   "source": [
    "暂时性记忆存储组件是一个非常重要的领域，它对Agent得作用非常巨大，因为只依靠context window来存储历史信息的做法实质上无法支撑长线操作的agent技术。\n",
    "因此tmp_storage实质上需要满足以下技术需求\n",
    "- 1.对于当前任务框架下的任务状态的描述，存储，可能以COT，TOT，TOG等多种方式作为描述媒介\n",
    "- 2.能够进行APIs的标准化存储的索引\n",
    "- 3.能够对历史对话进行向量化和存储\n",
    "    - 这点的处理流程可能会很复杂并存疑（历史信息的筛选和语义向量化存储的粒度应该是什么 or 该不该做）\n",
    "- 4.对于交流对话的meta，唯一标识符，索引\n",
    "- 5.对于历史信息查询后需要结构化回context得方式需要进一步的验证\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f31eb6-6978-46bb-bced-6d5cd63aec57",
   "metadata": {},
   "source": [
    "The temporary memory storage component is a highly critical area with a significant impact on agents, as relying solely on a context window to store historical information is fundamentally inadequate for long-term agent technology. Therefore, tmp_storage essentially needs to meet the following technical requirements:\n",
    "\n",
    "Description and storage of task states within the current task framework, possibly in various forms such as COT, TOT, TOG, serving as descriptive media.\n",
    "Ability to standardize and index APIs for storage.\n",
    "Ability to vectorize and store historical conversations.\n",
    "The processing flow for this point may be complex and subject to doubts (e.g., what granularity to use for filtering historical information and semantic vectorization storage, or whether it should be done).\n",
    "Unique identifiers and indexes for meta-information in communication dialogues.\n",
    "Further validation is needed for the structured return to the context after querying historical information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49513e8a-9e64-465d-ab14-bb7807c67a1d",
   "metadata": {},
   "source": [
    "#### ContextStructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fded18d-ad41-404b-89b2-50e27cfef1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "405c5af6-d729-4196-b249-8f477c03af9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "591b4c14-f8bf-4a65-8dca-23affd9e91ee",
   "metadata": {},
   "source": [
    "## Document Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75c6ff-f916-4083-a07a-9477b7dc63b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e04fc097-dfe9-4904-abec-53676202006c",
   "metadata": {},
   "source": [
    "### Unified DocumentLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed763adf-6805-448e-8102-2f739923e113",
   "metadata": {},
   "source": [
    "集成式的数据加载需要针对不同的数据类型和记录,以PDF为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a75c590-84fe-4654-8e9e-eeeccaf13f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF1=\"2306.08302.pdf\"\n",
    "PDF2=\"2307.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1322c7a5-4fe0-44bb-8fb0-85101ab2a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
    "from paddleocr import PaddleOCR\n",
    "import fitz\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class UnstructuredPaddlePDFLoader(UnstructuredFileLoader):\n",
    "    \"\"\"Loader that uses unstructured to load image files, such as PNGs and JPGs.\"\"\"\n",
    "\n",
    "    def _get_elements(self) -> List:\n",
    "        def pdf_ocr_txt(filepath, dir_path=\"tmp_files\"):\n",
    "            full_dir_path = os.path.join(os.path.dirname(filepath), dir_path)\n",
    "            if not os.path.exists(full_dir_path):\n",
    "                os.makedirs(full_dir_path)\n",
    "            ocr = PaddleOCR(use_angle_cls=True, lang=\"ch\", use_gpu=False, show_log=False)\n",
    "            doc = fitz.open(filepath)\n",
    "            txt_file_path = os.path.join(full_dir_path, f\"{os.path.split(filepath)[-1]}.txt\")\n",
    "            img_name = os.path.join(full_dir_path, 'tmp.png')\n",
    "            with open(txt_file_path, 'w', encoding='utf-8') as fout:\n",
    "                for i in range(doc.page_count):\n",
    "                    page = doc[i]\n",
    "                    text = page.get_text(\"\")\n",
    "                    fout.write(text)\n",
    "                    fout.write(\"\\n\")\n",
    "\n",
    "                    img_list = page.get_images()\n",
    "                    for img in img_list:\n",
    "                        pix = fitz.Pixmap(doc, img[0])\n",
    "                        if pix.n - pix.alpha >= 4:\n",
    "                            pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                        pix.save(img_name)\n",
    "                        result = ocr.ocr(img_name)\n",
    "                        result = list(filter(lambda x: x is not None, result))\n",
    "                        ocr_result = [i[1][0] for line in result for i in line]\n",
    "                        fout.write(\"\\n\".join(ocr_result))\n",
    "            if os.path.exists(img_name):\n",
    "                os.remove(img_name)\n",
    "            return txt_file_path\n",
    "\n",
    "        txt_file_path = pdf_ocr_txt(self.file_path)\n",
    "        from unstructured.partition.text import partition_text\n",
    "        return partition_text(filename=txt_file_path, **self.unstructured_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dfa6297-bcaf-4d8b-bed0-0cac37c53aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs length 5139\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredPaddlePDFLoader(PDF1, mode=\"elements\")\n",
    "docs = loader.load()\n",
    "print(\"docs length\",len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c2e8e-16b9-40ba-a648-e8038bbc2506",
   "metadata": {},
   "source": [
    "----\n",
    "> 从结果中我们也可以看到真对一个文档的信息提取能力。\n",
    "纯OCR以及Latex文件和PDF直接解析都不足以覆盖所有场景。\n",
    "因此对于文档的解析信息融合就是至关重要的问题\n",
    "\n",
    "- 1.OCR用于提取信息区块和大致信息内容，也要使用SAM来做图像图表的检测等；\n",
    "- 2.如果Latex/PostScript可用则使用信息直接融合，如果不可用则需要OCR增强\n",
    "- 3.逻辑上提取的图标部分需要使用多模态/Caption任务进行Meta信息生成/提取\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65ed6f-6d4d-4f8c-bf2a-35e9980c0753",
   "metadata": {},
   "source": [
    "在此基础之上统一读取器的读取策略需要在文件名+文件头双重验证。\n",
    "\n",
    "例如图像模态的读取需要从文件后缀\n",
    "（/jpg/png...）和图片的exif文件信息互相验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bdd482-2001-426f-8dc3-aac058e9b6be",
   "metadata": {},
   "source": [
    "### pecific Document Spilit & Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c7f84-86cd-4854-b262-b146af2e8fce",
   "metadata": {},
   "source": [
    "文档分词组件可谓至关重要。\n",
    "\n",
    "现有常规的如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42ea8ed-6ca5-471d-9db7-76212d37ba5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-07 17:03:45,361] [ WARNING] text_splitter.py:187 - Created a chunk of size 4833, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,361] [ WARNING] text_splitter.py:187 - Created a chunk of size 6767, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,362] [ WARNING] text_splitter.py:187 - Created a chunk of size 3461, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,362] [ WARNING] text_splitter.py:187 - Created a chunk of size 4814, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,362] [ WARNING] text_splitter.py:187 - Created a chunk of size 4909, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,362] [ WARNING] text_splitter.py:187 - Created a chunk of size 5088, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,363] [ WARNING] text_splitter.py:187 - Created a chunk of size 5236, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,363] [ WARNING] text_splitter.py:187 - Created a chunk of size 5612, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,363] [ WARNING] text_splitter.py:187 - Created a chunk of size 5437, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,363] [ WARNING] text_splitter.py:187 - Created a chunk of size 5304, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,364] [ WARNING] text_splitter.py:187 - Created a chunk of size 6246, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,364] [ WARNING] text_splitter.py:187 - Created a chunk of size 4908, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,364] [ WARNING] text_splitter.py:187 - Created a chunk of size 5494, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,364] [ WARNING] text_splitter.py:187 - Created a chunk of size 4895, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,365] [ WARNING] text_splitter.py:187 - Created a chunk of size 6145, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,365] [ WARNING] text_splitter.py:187 - Created a chunk of size 6354, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,365] [ WARNING] text_splitter.py:187 - Created a chunk of size 6345, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,365] [ WARNING] text_splitter.py:187 - Created a chunk of size 5033, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,366] [ WARNING] text_splitter.py:187 - Created a chunk of size 4724, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,366] [ WARNING] text_splitter.py:187 - Created a chunk of size 6222, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,366] [ WARNING] text_splitter.py:187 - Created a chunk of size 8977, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,367] [ WARNING] text_splitter.py:187 - Created a chunk of size 9003, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,367] [ WARNING] text_splitter.py:187 - Created a chunk of size 9023, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,367] [ WARNING] text_splitter.py:187 - Created a chunk of size 9092, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,367] [ WARNING] text_splitter.py:187 - Created a chunk of size 9149, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,368] [ WARNING] text_splitter.py:187 - Created a chunk of size 9041, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,368] [ WARNING] text_splitter.py:187 - Created a chunk of size 9478, which is longer than the specified 1000\n",
      "[2023-11-07 17:03:45,368] [ WARNING] text_splitter.py:187 - Created a chunk of size 9273, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n1\\nUnifying Large Language Models and\\nKnowledge Graphs: A Roadmap\\nShirui Pan, Senior Member, IEEE, Linhao Luo,\\nYufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, Fellow, IEEE\\nAbstract—Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language\\nprocessing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which\\noften fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example,\\nare structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge\\nfor inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing\\nmethods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs\\ntogether and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs\\nand KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the\\npre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text\\ngeneration, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually\\nbeneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and\\nsummarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.\\nIndex Terms—Natural Language Processing, Large Language Models, Generative Pre-Training, Knowledge Graphs, Roadmap,\\nBidirectional Reasoning.\\n✦\\n1\\nINTRODUCTION\\nLarge language models (LLMs)1 (e.g., BERT [1], RoBERTA\\n[2], and T5 [3]), pre-trained on the large-scale corpus,\\nhave shown great performance in various natural language\\nprocessing (NLP) tasks, such as question answering [4],\\nmachine translation [5], and text generation [6]. Recently,\\nthe dramatically increasing model size further enables the\\nLLMs with the emergent ability [7], paving the road for\\napplying LLMs as Artificial General Intelligence (AGI).\\nAdvanced LLMs like ChatGPT2 and PaLM23, with billions\\nof parameters, exhibit great potential in many complex\\npractical tasks, such as education [8], code generation [9]\\nand recommendation [10].\\n•\\nShirui Pan is with the School of Information and Communication Tech-\\nnology and Institute for Integrated and Intelligent Systems (IIIS), Griffith\\nUniversity, Queensland, Australia. Email: s.pan@griffith.edu.au;\\n•\\nLinhao Luo and Yufei Wang are with the Department of Data Sci-\\nence and AI, Monash University, Melbourne, Australia. E-mail: lin-\\nhao.luo@monash.edu, garyyufei@gmail.com.\\n•\\nChen Chen is with the Nanyang Technological University, Singapore. E-\\nmail: s190009@ntu.edu.sg.\\n•\\nJiapu Wang is with the Faculty of Information Technology, Beijing Uni-\\nversity of Technology, Beijing, China. E-mail: jpwang@emails.bjut.edu.cn.\\n•\\nXindong Wu is with the Key Laboratory of Knowledge Engineering with\\nBig Data (the Ministry of Education of China), Hefei University of\\nTechnology, Hefei, China; He is also affiliated with the Research Center\\nfor Knowledge Engineering, Zhejiang Lab, Hangzhou, China. Email:\\nxwu@hfut.edu.cn.\\n•\\nShirui Pan and Linhao Luo contributed equally to this work.\\n•\\nCorresponding Author: Xindong Wu.\\n1. LLMs are also known as pre-trained language models (PLMs).\\n2. https://openai.com/blog/chatgpt\\n3. https://ai.google/discover/palm2\\nFig. 1. Summarization of the pros and cons for LLMs and KGs. LLM\\npros: General Knowledge [11], Language Processing [12], Generaliz-\\nability [13]; LLM cons: Implicit Knowledge [14], Hallucination [15], In-\\ndecisiveness [16], Black-box [17], Lacking Domain-specific/New Knowl-\\nedge [18]. KG pros: Structural Knowledge [19], Accuracy [20], Decisive-\\nness [21], Interpretability [22], Domain-specific Knowledge [23], Evolv-\\ning Knowledge [24]; KG cons: Incompleteness [25], Lacking Language\\nUnderstanding [26], Unseen Facts [27].\\nDespite their success in many applications, LLMs have\\nbeen criticized for their lack of factual knowledge. Specif-\\nically, LLMs memorize facts and knowledge contained in\\nthe training corpus [14]. However, further studies reveal\\nthat LLMs are not able to recall facts and often experience\\nhallucinations by generating statements that are factually\\nincorrect [15], [28]. For example, LLMs might say “Ein-\\n0000–0000/00$00.00 © 2021 IEEE\\narXiv:2306.08302v2  [cs.CL]  20 Jun 2023' metadata={'source': './tmp_files/2306.08302.pdf.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "raw_documents = TextLoader('./tmp_files/2306.08302.pdf.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a67837-f064-4ec7-aa66-aca9e75633c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Knowledge Graphs (KGs)\\nCons:\\nPros:\\nImplicit Knowledge\\nStructural Knowledge\\nHallucination\\nAccuracy\\nDecisiveness\\nIndecisiveness\\nBlack-box\\nInterpretability\\nLacking Domain-\\nDomain-specific Knowledge\\nspecific/New Knowledge\\nEvolving Knowledge\\nCons:\\nPros:\\nIncompleteness\\nGeneral Knowledge\\nLacking Language\\n Language Processing\\nUnderstanding\\nGeneralizability\\nUnseen Facts\\nLarge Language Models (LLMs)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\\n2\\nstein discovered gravity in 1687” when asked, “When did\\nEinstein discover gravity?”, which contradicts the fact that\\nIsaac Newton formulated the gravitational theory. This issue\\nseverely impairs the trustworthiness of LLMs.\\nAs black-box models, LLMs are also criticized for their\\nlack of interpretability. LLMs represent knowledge implic-\\nitly in their parameters. It is difficult to interpret or validate\\nthe knowledge obtained by LLMs. Moreover, LLMs perform\\nreasoning by a probability model, which is an indecisive\\nprocess [16]. The specific patterns and functions LLMs\\nused to arrive at predictions or decisions are not directly\\naccessible or explainable to humans [17]. Even though some\\nLLMs are equipped to explain their predictions by applying\\nchain-of-thought [29], their reasoning explanations also suf-\\nfer from the hallucination issue [30]. This severely impairs\\nthe application of LLMs in high-stakes scenarios, such as\\nmedical diagnosis and legal judgment. For instance, in a\\nmedical diagnosis scenario, LLMs may incorrectly diagnose\\na disease and provide explanations that contradict medical\\ncommonsense. This raises another issue that LLMs trained\\non general corpus might not be able to generalize well\\nto specific domains or new knowledge due to the lack of\\ndomain-specific knowledge or new training data [18].\\nTo address the above issues, a potential solution is to in-\\ncorporate knowledge graphs (KGs) into LLMs. Knowledge\\ngraphs (KGs), storing enormous facts in the way of triples,\\ni.e., (head entity, relation, tail entity), are a structured and\\ndecisive manner of knowledge representation (e.g., Wiki-\\ndata [20], YAGO [31], and NELL [32]). KGs are crucial for\\nvarious applications as they offer accurate explicit knowl-\\nedge [19]. Besides, they are renowned for their symbolic\\nreasoning ability [22], which generates interpretable results.\\nKGs can also actively evolve with new knowledge contin-\\nuously added in [24]. Additionally, experts can construct\\ndomain-specific KGs to provide precise and dependable\\ndomain-specific knowledge [23].\\nNevertheless, KGs are difficult to construct [33], and\\ncurrent approaches in KGs [25], [27], [34] are inadequate\\nin handling the incomplete and dynamically changing na-\\nture of real-world KGs. These approaches fail to effectively\\nmodel unseen entities and represent new facts. In addition,\\nthey often ignore the abundant textual information in KGs.\\nMoreover, existing methods in KGs are often customized for\\nspecific KGs or tasks, which are not generalizable enough.\\nTherefore, it is also necessary to utilize LLMs to address the\\nchallenges faced in KGs. We summarize the pros and cons\\nof LLMs and KGs in Fig. 1, respectively.\\nRecently, the possibility of unifying LLMs with KGs has\\nattracted increasing attention from researchers and practi-\\ntioners. LLMs and KGs are inherently interconnected and\\ncan mutually enhance each other. In KG-enhanced LLMs,\\nKGs can not only be incorporated into the pre-training and\\ninference stages of LLMs to provide external knowledge\\n[35]–[37], but also used for analyzing LLMs and provid-\\ning interpretability [14], [38], [39]. In LLM-augmented KGs,\\nLLMs have been used in various KG-related tasks, e.g., KG\\nembedding [40], KG completion [26], KG construction [41],\\nKG-to-text generation [42], and KGQA [43], to improve the\\nperformance and facilitate the application of KGs. In Syn-\\nergized LLM + KG, researchers marries the merits of LLMs\\nand KGs to mutually enhance performance in knowledge\\nrepresentation [44] and reasoning [45], [46]. Although there\\nare some surveys on knowledge-enhanced LLMs [47]–[49],\\nwhich mainly focus on using KGs as an external knowledge\\nto enhance LLMs, they ignore other possibilities of integrat-\\ning KGs for LLMs and the potential role of LLMs in KG\\napplications.\\nIn this article, we present a forward-looking roadmap for\\nunifying both LLMs and KGs, to leverage their respective\\nstrengths and overcome the limitations of each approach,\\nfor various downstream tasks. We propose detailed cate-\\ngorization, conduct comprehensive reviews, and pinpoint\\nemerging directions in these fast-growing fields. Our main\\ncontributions are summarized as follows:\\n1)\\nRoadmap. We present a forward-looking roadmap\\nfor integrating LLMs and KGs. Our roadmap,\\nconsisting of three general frameworks to unify\\nLLMs and KGs, namely, KG-enhanced LLMs, LLM-\\naugmented KGs, and Synergized LLMs + KGs, pro-\\nvides guidelines for the unification of these two\\ndistinct but complementary technologies.\\n2)\\nCategorization and review. For each integration\\nframework of our roadmap, we present a detailed\\ncategorization and novel taxonomies of research\\non unifying LLMs and KGs. In each category, we\\nreview the research from the perspectives of differ-\\nent integration strategies and tasks, which provides\\nmore insights into each framework.\\n3)\\nCoverage of emerging advances. We cover the\\nadvanced techniques in both LLMs and KGs. We\\ninclude the discussion of state-of-the-art LLMs like\\nChatGPT and GPT-4 as well as the novel KGs e.g.,\\nmulti-modal knowledge graphs.\\n4)\\nSummary of challenges and future directions. We\\nhighlight the challenges in existing research and\\npresent several promising future research direc-\\ntions.\\nThe rest of this article is organized as follows. Section\\n2 first explains the background of LLMs and KGs. Section\\n3 introduces the roadmap and the overall categorization of\\nthis article. Section 4 presents the different KGs-enhanced\\nLLM approaches. Section 5 describes the possible LLM-\\naugmented KG methods. Section 6 shows the approaches\\nof synergizing LLMs and KGs. Section 7 discusses the\\nchallenges and future research directions. Finally, Section 8\\nconcludes this paper.\\n2\\nBACKGROUND\\nIn this section, we will first briefly introduce a few rep-\\nresentative large language models (LLMs) and discuss the\\nprompt engineering that efficiently uses LLMs for varieties\\nof applications. Then, we illustrate the concept of knowl-\\nedge graphs (KGs) and present different categories of KGs.\\n2.1\\nLarge Language models (LLMs)\\nLarge language modes (LLMs) pre-trained on large-scale\\ncorpus have shown great potential in various NLP tasks\\n[13]. As shown in Fig. 3, most LLMs derive from the Trans-\\nformer design [50], which contains the encoder and decoder\\nmodules empowered by a self-attention mechanism. Based' metadata={'source': './tmp_files/2306.08302.pdf.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e813c98d-f135-40e0-9f4b-652a0b8ed952",
   "metadata": {},
   "source": [
    "> 在很多层面一个固定的chunk_size和overlap实质上会浪费大量的语义。召回concat context的时候也会浪费大量空间。\n",
    "因此根据语义来进行不同粒度的拆分逻辑会有更大的前景。\n",
    "\n",
    "但首先要解决的问题是\n",
    "- 0.设定最大词汇数量下，根据语义来进行分段\n",
    "- 1.chunk内部的上下文关联问题\n",
    "- 2.文章和chunk之间的关联问题\n",
    "- 3.chunk和内部语句之间的关联问题\n",
    "- 4.不同粒度之间的语义是否能够完全表征的问题\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03d196-9d1c-4747-b40d-9572d040e4b2",
   "metadata": {},
   "source": [
    "#### 因此在新的框架下我们需要真对这部分建立新的索引体系。\n",
    "索引头包含了\n",
    "文件级别\n",
    "- 1.meta 信息的向量化\n",
    "- 2.全文summary的信息的向量化\n",
    "- 3.多模态信息列表以及对应的向量化\n",
    "语义分段的chunk级别\n",
    "- 1.语义分段的chunk向量化\n",
    "- 2.根据段落的再次语义分句的向量化\n",
    "- 3.chunk中的整体实体关系概念抽取\n",
    "- 4.chunk中的单语句级别实体关系概念抽取，语义向量化，唯一标识符建立。\n",
    "- 5.chunk中的句组语义向量化\n",
    "在以上五个部分中，chunk需要进行一次总体的抽取和索引处理，chunk语义拆分后的语句还需要进行进一步的实体关系和概念抽取。\n",
    "结果经过剪枝查重后，新增部分进入总体的库中\n",
    "需要进行的实验设计\n",
    "- 1.语义分段的语义转换性能，相比静态chunksize——overlap有多大提升\n",
    "- 2.语义分段后的不同字数，转换向量质量是否有影响\n",
    "- 3.多向量查询在向量数据库层面的支持情况/chroma or zilliz\n",
    "- 4.不同粒度对语义特征的表征能力和空间是否一致。\n",
    "....\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1062b8-a16e-43e1-a26b-596e40c0b4e6",
   "metadata": {},
   "source": [
    "Therefore, in the new framework, we need to establish a new indexing system for this part. The index header includes:\n",
    "\n",
    "At the file level:\n",
    "- 1. Vectorization of meta information.\n",
    "- 2. Vectorization of full-text summary information.\n",
    "- 3. List of multi-modal information and their corresponding vectorization.\n",
    "\n",
    "At the semantic chunk level:\n",
    "- 1. Vectorization of semantic chunk segments.\n",
    "- 2. Vectorization based on further semantic sub-sentences within paragraphs.\n",
    "- 3. Extraction of overall entity relations and concepts within the chunk.\n",
    "- 4. Extraction of entity relations and concepts at the single-sentence level within the chunk, semantic vectorization, and unique identifier establishment.\n",
    "- 5. Semantic vectorization of sentence groupings within the chunk.\n",
    "\n",
    "Within these five sections, chunks require an initial extraction and indexing process, and sentences resulting from semantic chunk segmentation need further extraction of entity relations and concepts.\n",
    "\n",
    "After pruning and deduplication, the newly added components enter the overall repository.\n",
    "\n",
    "Experimental design is required for:\n",
    "- 1. Performance comparison of semantic conversion regarding different chunk sizes and overlaps compared to static chunk sizes and overlaps.\n",
    "- 2. Evaluation of the impact of different word counts after semantic chunking on vector quality.\n",
    "- 3. Support for multi-vector queries at the vector database level using technologies like Chroma or Zilliz.\n",
    "- 4. Assessment of the consistency of the representation capabilities and space for different granularities in semantic features.\n",
    "\n",
    "And so on..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c23ab0b2-5235-4b9f-98d3-ef793145210a",
   "metadata": {},
   "source": [
    "#### Layout Parser\n",
    "在此基础上常规的读取方式存在较大的信息结构化不完整的问题：\n",
    "因此需要介入Layout解析 Baseline：[repo](https://github.com/Layout-Parser/layout-parser)\n",
    "引入新的内容解析策略\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9069a13c-6bbd-47ed-a3a0-5390d6993a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ba16d-0b04-4070-8493-ac54ef8dc0b7",
   "metadata": {},
   "source": [
    "### MetaParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe854b-3fe5-4fbb-959b-f0a0f509b3a3",
   "metadata": {},
   "source": [
    "meta信息的解析，需要和loader构成同一组件，并且需要能够反馈验证文件类型和转换策略。最好将整个过程编码化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fcac2-d6dc-4ebd-995d-119ae053034d",
   "metadata": {},
   "source": [
    "即将文件类型编码，通信中不展示具体的文件编码和解析策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5c069-d991-435c-980b-56d4ed854fec",
   "metadata": {},
   "source": [
    "## Compute System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae3b22-9904-4dc2-8648-31cbcc592080",
   "metadata": {},
   "source": [
    "### StructuredChunks Define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc7c441-0bfa-47bb-974f-5cd2454783cc",
   "metadata": {},
   "source": [
    "在结构化知识对大模型推理的增强过程中主要分为两种手段\n",
    "\n",
    "1.动态知识融合\n",
    "这种方法一般会要求要求LLM对知识和问题进行编码。进入到推理层，同时在attention层面进行重映射，但此类方法的设计难度较大，且通常无法在数据层面给予更大范围的支持。目前注入QAGNN类图神经网络基本无法达到较好的结果\n",
    "\n",
    "2.查询增强的知识融合\n",
    "这类方法一般会在图的遍历查询上下功夫，此种方法一般在网络外，在查询层面，将较大的图谱进行外挂处理。然后在Retrieving流程层面重新设计\n",
    "\n",
    "在定义层面要考虑目前生成和运算的困境\n",
    "1.数学表达式\n",
    "2.逻辑表达式\n",
    "3.实体和关系定义\n",
    "4.存储结构，体系结构\n",
    "5.时间空间序列\n",
    "\n",
    "3,数据库层面需求，多向量，多字段，通过节点信息和图数据库相互索引。\n",
    "\n",
    "考虑\n",
    "1.单个实体和关系定义需不需要进行向量化计算和存储。\n",
    "如果进行计算存储可能会造成规模爆炸即产生：\n",
    "图的存储可能会涉及到词在无上下文的情况下进行向量化毫无意义。\n",
    "可能最小的向量计算单元为关系级别\n",
    "因此在这个框架下\n",
    "可能是节点不需要进行响亮持久化存储和计算。\n",
    "而将关系级别的词进行向量化计算。\n",
    "\n",
    "1.在chunk阶段分词需要尽可能按照段落语义句子来拆分（不能）。因此不同长度的chunk是否可以统一进行embedding计算\n",
    "2.尝试设计流程\n",
    "\t1，对实体节点，关系进行提取，并支持neo4j，nebula操作\n",
    "\t2，对关系进行重组织后进行relation和向量化计算，以及embedding后的互相索引。\n",
    "\t3，节点和向量化信息的存储和Retrieving设计\n",
    "\t4.数学表达式：\n",
    "\t\tOCR 数学表达式提取\n",
    "\t\tLatex 数学表达式\n",
    "\t5.重构现有的loader\n",
    "\t\t图片和图片Meta信息提取\n",
    "      \t\t图片赋予UUID\n",
    "\t\tOCR Caption 图像提取信息结构化\n",
    "\t\tmeta信息结构化\n",
    "\t6.逻辑表达式：\n",
    "\t\t对于现有的数理逻辑推断语言进行支持，以支持更多的walframe/z3这样的推断引擎。\n",
    "\t7.RAG Pipeline Definition could reinforcement knowledge interaction with knowledge illustration limit in specific area or space \n",
    "在这个情况下单个chunk的relation提取就变的至关重要。并且单个chunk需要对应多个向量，需要探究向量数据库的改造，同步的在图关系侧也需要在relation层面加入对向量id的反响索引\n",
    "\n",
    "\n",
    "原文语义分割级别的chunk，作为在向量体系中被一级索引的最基本单元。\n",
    "设计至关重要，在整体的索引关系上大致为(暂时性定义)\n",
    "\n",
    "1.（Class Document）文档/文件级别：\n",
    "- 关系性索引唯一标识符-（UUID）\n",
    "- meta\n",
    "- meta embedding\n",
    "- raw content\n",
    "\n",
    "2.(Class StructuredChunk)针真对raw content得splited chunks级别\n",
    "\n",
    "- chunk唯一标识索引（UUID）\n",
    "- chunk raw content\n",
    "- chunk summary content（auto meta）\n",
    "- 模态标识符\n",
    "- 父文档唯一标识符（UUID）\n",
    "- chunk level 定制化的实体抽取结果列表\n",
    "- chunk level 定制化的关系抽取结果列表\n",
    "- chunk level 定制化的三元组抽取结果列表\n",
    "\n",
    "- chunk level 定制化的逻辑表达式抽取结果列表\n",
    "- chunk level 数学表达式抽取结果列表\n",
    "- ...\n",
    "\n",
    "3.(Class StructuredSentence)真对单个chunk语义分句级别\n",
    "\n",
    "- 实体关系，如果在chunk级别的实体关系中已经出现则不添加\n",
    "- 逻辑表达式，同理，需要和chunk级别做重复性和逻辑冲突性校验\n",
    "- 数学表达式\n",
    "- 语句向量化\n",
    "- 父chunk唯一标识符（UUID）\n",
    "- 父文档唯一标识符（UUID）\n",
    "\n",
    "****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d487932-b3c4-41c1-bb4f-4f24673466f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83293211-3a7b-45ad-a12e-2f46de598fbe",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a47d55-f62b-45b7-9fe0-dadd30a86f62",
   "metadata": {},
   "source": [
    "模块作为信息处理的基本组件\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838318c-1f89-4720-9b3c-a74573ea4baf",
   "metadata": {},
   "source": [
    "### Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460e43b-70b8-4623-9433-0b6c9ad5646a",
   "metadata": {},
   "source": [
    "流作为组装模块的最终组件负责暴漏最后的接口\n",
    "，负责整个单个Pipeline的任务调度，需要对任务的计算图进行优化然后对整体执行做出拆分，多线程执行\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596be9a-583e-4a5d-92d7-bbd9b0acdeaf",
   "metadata": {},
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adb72b-ad17-4711-90ff-4017f69f16f0",
   "metadata": {},
   "source": [
    "作为定制单数据查询链路的模块应当视为每种模态数据库查询的唯一模块：\n",
    "例如结构化数据对应的关系型数据库，应当有独立的Retriever基类\n",
    "各个数据库再进行对于结构化信息查询的关系型数据库基类进行继承实现\n",
    "同理图也是。\n",
    "参考实现：\n",
    "[langchain retriever](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/schema/retriever.py)\n",
    "但在图的定义过程中产生了显著的区别即我们要求\n",
    "`ananke/src/ananke/data.py`中定义的节点和关系分别有符号系统和语义向量系统的双重表示。这就意味着我们需要做两方面的探索\n",
    "- 1.在现有技术框架下实现：\n",
    "    - 1.向量对应UUID（1），存入向量数据库\n",
    "    - 2.语义的description对应UUID（1），存入Graph\n",
    "    - 3.图实体节点和关系都要添加对以上UUID（1）作为索引。\n",
    "- 2.变更技术框架\n",
    "    - 1.可以多向量索引的底座\n",
    "    - 2.基于NaLLM项目的图查询语言（以）语言和向量体系兼容试验。\n",
    "    - 3.基于在以上两个语言体系下附加基于唯一索引的关系数据库体系（Easy）。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fead7e9-e3f1-4d56-91a7-8ac023872422",
   "metadata": {},
   "source": [
    "## Storage System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b638ac6-0b2d-46c2-a7e9-4beda3125cc9",
   "metadata": {},
   "source": [
    "### VectorDB\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.\n",
    "![](https://python.langchain.com/assets/images/vector_stores-9dc1ecb68c4cb446df110764c9cc07e0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d95fef-3eba-49c4-b268-c3d56d0dbd8a",
   "metadata": {},
   "source": [
    "在向量存储体系中\n",
    "> 如同 langchain的定义一样'langchain/libs/langchain/langchain/schema/vectorstore.py'[https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/schema/vectorstore.py](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/schema/vectorstore.py)所展示的定义过程接口定义：\n",
    "\n",
    "`VectorStore`的抽象基类，用于处理文本向量存储。它提供了一些基本的方法来处理文本向量，例如添加文本、删除文本、搜索文本等。以下是`VectorStore`类的主要接口和功能：\n",
    "```python\n",
    "1. `add_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) -> List[str]`: 将文本添加到向量存储中。参数`texts`是一个可迭代的字符串列表，`metadatas`是一个可选的元数据列表，`kwargs`是向量存储特定的参数。返回添加到向量存储中的文本的ID列表。\n",
    "\n",
    "2. `delete(ids: Optional[List[str]] = None, **kwargs: Any) -> Optional[bool]`: 根据向量ID或其他条件删除文本。参数`ids`是一个可选的ID列表，`kwargs`是其他关键字参数，子类可能使用。返回一个布尔值，表示删除是否成功，如果未实现则返回`None`。\n",
    "\n",
    "3. `adelete(ids: Optional[List[str]] = None, **kwargs: Any) -> Optional[bool]`: 异步删除文本。参数和返回值与`delete`方法相同。\n",
    "\n",
    "4. `aadd_texts(texts: Iterable[str], metadatas: Optional[List[dict]] = None, **kwargs: Any) -> List[str]`: 异步添加文本到向量存储。参数和返回值与`add_texts`方法相同。\n",
    "\n",
    "5. `add_documents(documents: List[Document], **kwargs: Any) -> List[str]`: 将文档添加到向量存储中。参数`documents`是一个文档列表，`kwargs`是其他关键字参数。返回添加到向量存储中的文本的ID列表。\n",
    "\n",
    "6. `aadd_documents(documents: List[Document], **kwargs: Any) -> List[str]`: 异步将文档添加到向量存储中。参数和返回值与`add_documents`方法相同。\n",
    "\n",
    "7. `search(query: str, search_type: str, **kwargs: Any) -> List[Document]`: 使用指定的搜索类型返回与查询最相似的文档。参数`query`是要搜索的查询字符串，`search_type`是搜索类型（可以是`similarity`或`mmr`），`kwargs`是其他关键字参数。返回一个文档列表。\n",
    "\n",
    "8. `asearch(query: str, search_type: str, **kwargs: Any) -> List[Document]`: 异步执行`search`方法。\n",
    "\n",
    "9. `similarity_search(query: str, k: int = 4, **kwargs: Any) -> List[Document]`: 使用相似性搜索返回与查询最相似的文档。参数`query`是要搜索的查询字符串，`k`是要返回的文档数量，默认为4，`kwargs`是其他关键字参数。返回一个文档列表。\n",
    "\n",
    "10. `asimilarity_search(query: str, k: int = 4, **kwargs: Any) -> List[Document]`: 异步执行`similarity_search`方法。\n",
    "\n",
    "11. `similarity_search_with_score(query: str, *args: Any, **kwargs: Any) -> List[Tuple[Document, float]]`: 使用相似性搜索返回与查询最相似的文档及其相似度分数。参数和返回值与`similarity_search`方法相同。\n",
    "\n",
    "12. `asimilarity_search_with_score(query: str, *args: Any, **kwargs: Any) -> List[Tuple[Document, float]]`: 异步执行`similarity_search_with_score`方法。\n",
    "\n",
    "13. `similarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any) -> List[Tuple[Document, float]]`: 使用相似性搜索返回与查询最相似的文档及其相关性分数。参数和返回值与`similarity_search_with_score`方法相同。\n",
    "\n",
    "14. `asimilarity_search_with_relevance_scores(query: str, k: int = 4, **kwargs: Any) -> List[Tuple[Document, float]]`: 异步执行`similarity_search_with_relevance_scores`方法。\n",
    "\n",
    "15. `similarity_search_by_vector(query: List[float], k: int = 4, **kwargs: Any) -> List[Document]`: 使用向量搜索返回与查询最相似的文档。参数`query`是要搜索的查询向量，`k`是要返回的文档数量，默认为4，`kwargs`是其他关键字参数。返回一个文档列表。\n",
    "\n",
    "16. `asimilarity_search_by_vector(query: List[float], k: int = 4, **kwargs: Any) -> List[Document]`: 异步执行`similarity_search_by_vector`方法。\n",
    "\n",
    "17. `max_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any) -> List[Document]`: 使用最大边际相关性搜索返回与查询最相关的文档。参数`query`是要搜索的查询字符串，`k`是要返回的文档数量，默认为4，`fetch_k`是要传递给MMR算法的文档数量，默认为20，`lambda_mult`是结果多样性的度量，范围为0到1，默认为0.5，`kwargs`是其他关键字参数。返回一个文档列表。\n",
    "\n",
    "18. `amax_marginal_relevance_search(query: str, k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any) -> List[Document]`: 异步执行`max_marginal_relevance_search`方法。\n",
    "\n",
    "19. `max_marginal_relevance_search_by_vector(query: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any) -> List[Document]`: 使用向量搜索返回与查询最相关的文档。参数和返回值与`max_marginal_relevance_search`方法相同。\n",
    "\n",
    "20. `amax_marginal_relevance_search_by_vector(query: List[float], k: int = 4, fetch_k: int = 20, lambda_mult: float = 0.5, **kwargs: Any) -> List[Document]`: 异步执行`max_marginal_relevance_search_by_vector`方法。\n",
    "\n",
    "21. `from_documents(cls: Type[VST], documents: List[Document], embedding: Embeddings, **kwargs: Any) -> VST`: 从文档和嵌入初始化`VectorStore`。参数`documents`是一个文档列表，`embedding`是嵌入对象，`kwargs`是其他关键字参数。返回一个`VectorStore`实例。\n",
    "\n",
    "22. `afrom_documents(cls: Type[VST], documents: List[Document], embedding: Embeddings, **kwargs: Any) -> VST`: 异步执行`from_documents`方法。\n",
    "\n",
    "23. `from_texts(cls: Type[VST], texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) -> VST`: 从文本和嵌入初始化`VectorStore`。参数`texts`是一个字符串列表，`embedding`是嵌入对象，`metadatas`是一个可选的元数据列表，`kwargs`是其他关键字参数。返回一个`VectorStore`实例。\n",
    "\n",
    "24. `afrom_texts(cls: Type[VST], texts: List[str], embedding: Embeddings, metadatas: Optional[List[dict]] = None, **kwargs: Any) -> VST`: 异步执行`from_texts`方法。\n",
    "\n",
    "25. `_get_retriever_tags() -> List[str]`: 获取检索器的标签。返回一个字符串列表。\n",
    "\n",
    "26. `as_retriever(**kwargs: Any) -> VectorStoreRetriever`: 返回一个`VectorStoreRetriever`实例，该实例从当前`VectorStore`初始化。参数`kwargs`是其他关键字参数。返回一个`VectorStoreRetriever`实例。\n",
    "\n",
    "此外，代码中还定义了一个名为`VectorStoreRetriever`的基类，用于处理`VectorStore`的检索。它继承自`BaseRetriever`类，并提供了一些用于检索文档的方法。\n",
    "```\n",
    "\n",
    "\n",
    "存在的问题\n",
    "- 1.不支持多向量构建体系，在数据库层面的联系需要更紧密\n",
    "- 2.异步执行被过度的接口化了，本质上这个需要在接口层面无感，或参数化\n",
    "- 3.抽象逻辑不合理在之后的向量化存储中，需要兼顾短时和持久化记忆的双重存储需求因此需要在基础类的基础上延伸新的短期组件和长期组件。\n",
    "- 4.对应大的结构化chunk数据结构这个存储接口也需要大改\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac8e46-3fb9-4470-a675-500bcbdb196e",
   "metadata": {},
   "source": [
    "#### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae6caff-dc10-4bf0-9e7a-5eacd1493d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "197c88ac-61d8-4649-a7d4-6dbea18b1d6a",
   "metadata": {},
   "source": [
    "### Graph(Structure)DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0a019-0647-4dbd-ba6a-f95f2e2eabef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd4c35cf-ff6b-4773-880f-5773f95ce0f0",
   "metadata": {},
   "source": [
    "#### Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18dd3b-a990-4cad-a77b-033b58741e01",
   "metadata": {},
   "source": [
    "存储过程可以套用 llamaindex [neo4j](https://docs.llamaindex.ai/en/stable/examples/index_structs/knowledge_graph/Neo4jKGIndexDemo.html)\n",
    "语义图的查询语言交互可以借鉴[NaLLM](https://github.com/neo4j/NaLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b22b36e-6a3a-4801-a108-302452d6980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores import Neo4jGraphStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    ")\n",
    "username = \"neo4j\"\n",
    "password = \"retractor-knot-thermocouples\"\n",
    "url = \"bolt://44.211.44.239:7687\"\n",
    "database = \"neo4j\"\n",
    "# graph_store = Neo4jGraphStore(\n",
    "#     username=username,\n",
    "#     password=password,\n",
    "#     url=url,\n",
    "#     database=database,\n",
    "# )\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "# # NOTE: can take a while!\n",
    "# index = KnowledgeGraphIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     max_triplets_per_chunk=2,\n",
    "#     service_context=service_context,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc525c4-a50b-4eca-ab33-872daf2ab4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff0854a-d93b-4257-a71f-16a021a5311c",
   "metadata": {},
   "source": [
    "#### Nebula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ecd716-ebc0-46a3-94e6-4c79b5a8dda0",
   "metadata": {},
   "source": [
    "同理，这也就要求在上层需要保持抽象的一致性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5dd19-1f06-40c0-9673-b7d4012ba21a",
   "metadata": {},
   "source": [
    "### RelationDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea6401-bcf2-401e-bb93-94aa5f5a12a7",
   "metadata": {},
   "source": [
    "参考实现[Langchain Base Storage](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/schema/storage.py)\n",
    "\n",
    "关系型数据库的基类实现没有什么特别的trick，\n",
    "主要聚焦于基类对配置的继承 具体实现应当从：\n",
    "`ananke/src/ananke/db/__init__.py`开始继承配置项然后开始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f35f3-a066-4a60-988d-83fd037cb8cc",
   "metadata": {},
   "source": [
    "#### Sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e088b-244c-494f-9a2a-3e274bff4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "同理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd9494-3c53-43db-b56e-4a23dcb53b52",
   "metadata": {},
   "source": [
    "#### MariaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78de8d-bd22-4196-b51e-a018c0e3f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "同理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4964f85-3ecf-4efa-b16a-9ff89440f35c",
   "metadata": {},
   "source": [
    "# Final Interaction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fb8d4-baa7-485c-93ed-468ad84f58fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
