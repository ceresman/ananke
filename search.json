{
    "pdf_id": "",
    "self": {
        "chunks": [],
        "phase": []
    },
    "relevant": {
        "chunks": [
            {
                "id": "19",
                "metadata": {
                    "chunk_id": 19,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 19
                },
                "dis": 0.18000860254928008,
                "documents": "Figure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves \\(85 \\mathrm{~F} 1\\) in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models. \\begin{tabular}{lcccccc}\\hline & \\begin{tabular}{c} SuperGLUE \\\\ Average \\end{tabular} & \\begin{tabular}{c} BoolQ \\\\ Accuracy \\end{tabular} & \\begin{tabular}{c} CB \\\\ Accuracy \\end{tabular} & \\begin{tabular}{c} CB \\\\ F1 \\end{tabular} & \\begin{tabular}{c} COPA \\\\ Accuracy \\end{tabular} & \\begin{tabular}{c} RTE \\\\ Accuracy \\end{tabular} \\\\ \\hline Fine-tuned SOTA & \\(\\mathbf{8 9 . 0}\\) & \\(\\mathbf{9 1 . 0}\\) & \\(\\mathbf{9 6 . 9}\\) & \\(\\mathbf{9 3 . 9}\\) & \\(\\mathbf{9 4 . 8}\\) & \\(\\mathbf{9 2 . 5}\\) \\\\ Fine-tuned BERT-Large & 69.0 & 77.4 & 83.6 & 75.7 & 70.6 & 71.7 \\\\ GPT-3 Few-Shot & 71.8 & 76.4 & 75.6 & 52.0 & 92.0 & 69.0 \\\\ \\hline & & & & & & \\\\ & & & & & & \\\\ & WiC & WSC & MultiRC & MultiRC & ReCoRD & ReCoRD \\\\ & Accuracy & Accuracy & Accuracy & F1a & Accuracy & F1 \\\\ \\hline Fine-tuned SOTA & \\(\\mathbf{7 6 . 1}\\) & \\(\\mathbf{9 3 . 8}\\) & \\(\\mathbf{6 2 . 3}\\) & \\(\\mathbf{8 8 . 2}\\) & \\(\\mathbf{9 2 . 5}\\) & \\(\\mathbf{9 3 . 3}\\) \\\\ Fine-tuned BERT-Large & 69.6 & 64.6 & 24.1 & 70.0 & 71.3 & 72.0 \\\\ GPT-3 Few-Shot & 49.4 & 80.1 & 30.5 & 75.4 & 90.2 & 91.1 \\\\ \\hline\\end{tabular} Table 3.8: Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates. 19"
            },
            {
                "id": "14",
                "metadata": {
                    "chunk_id": 14,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 14
                },
                "dis": 0.1835443442271314,
                "documents": "TriviaQA Figure 3.3: On TriviaQA GPT3's performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG \\(\\left[\\mathrm{LPP}^{+} 20\\right]\\) and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting. On Natural Questions (NQs) GPT-3 achieves 14.6\\% in the zero-shot setting, \\(23.0 \\%\\) in the one-shot setting, and \\(29.9 \\%\\) in the few-shot setting, compared to \\(36.6 \\%\\) for fine-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very fine-grained knowledge on Wikipedia specifically which could be testing the limits of GPT-3's capacity and broad pretraining distribution. Overall, on one of the three datasets GPT-3's one-shot matches the open-domain fine-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using fine-tuning. On all 3 datasets, we find that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reflecting the idea that model capacity translates directly to more 'knowledge' absorbed in the parameters of the model. 3.3 Translation For GPT-2 a filter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this filtering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement. As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3's training data is still primarily English (93\\% by word count), it also includes 7\\% of text in other languages. These languages are documented in the supplemental material. In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian. Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren't strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data. Results are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for 14"
            },
            {
                "id": "21",
                "metadata": {
                    "chunk_id": 21,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 21
                },
                "dis": 0.18355004231312688,
                "documents": "Figure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of \\(1.2 \\%\\) ). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix. whether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56\\%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset \\(\\left[\\mathrm{NWD}^{+}\\right.\\)19]. ANLI is a difficult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting ( \\(\\sim 33 \\%\\) ), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results for all rounds can be found in Appendix H. These results on both RTE and ANLI suggest that NLI is still a very difficult task for language models and they are only just beginning to show signs of progress. 3.9 Synthetic and Qualitative Tasks One way to probe GPT-3's range of abilities in the few-shot (or zero- and one-shot) setting is to give it tasks which require it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3's ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3's ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models. 3.9.1 Arithmetic To test GPT-3's ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language: - 2 digit addition (2D+) - The model is asked to add two integers sampled uniformly from \\([0,100)\\), phrased in the form of a question, e.g. \"Q: What is 48 plus 76? A: 124.\" - 2 digit subtraction (2D-) - The model is asked to subtract two integers sampled uniformly from \\([0,100)\\); the answer may be negative. Example: \"Q: What is 34 minus 53? A: -19\". - 3 digit addition (3D+) - Same as 2 digit addition, except numbers are uniformly sampled from \\([0,1000)\\). 21"
            },
            {
                "id": "18",
                "metadata": {
                    "chunk_id": 18,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 18
                },
                "dis": 0.18810838211136838,
                "documents": "\\begin{tabular}{lllllll}\\hline Setting & CoQA & DROP & QuAC & SQuADv2 & RACE-h & RACE-m \\\\ \\hline Fine-tuned SOTA & \\(\\mathbf{9 0 . 7}^{a}\\) & \\(\\mathbf{8 9 . 1}^{b}\\) & \\(\\mathbf{7 4 . 4}^{c}\\) & \\(\\mathbf{9 3 . 0}^{d}\\) & \\(\\mathbf{9 0 . 0}^{e}\\) & \\(\\mathbf{9 3 . 1}^{e}\\) \\\\ GPT-3 Zero-Shot & 81.5 & 23.6 & 41.5 & 59.5 & 45.5 & 58.4 \\\\ GPT-3 One-Shot & 84.0 & 34.3 & 43.3 & 65.4 & 45.9 & 57.4 \\\\ GPT-3 Few-Shot & 85.0 & 36.5 & 44.3 & 69.8 & 46.8 & 58.1 \\\\ \\hline\\end{tabular} Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy. \\({ }^{a}\\left[\\mathrm{JZC}^{+} \\text {19] }\\right]^{b}[\\mathrm{JN} 20]{ }^{c}[\\mathrm{AI} 19]{ }^{d}[\\mathrm{QIA} 20]{ }^{e}\\left[\\mathrm{SPP}^{+}\\right.\\)19] fine-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over \\(10 \\%\\) worse than human performance, but GPT-3's few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details. \\(\\mathrm{ARC}\\left[\\mathrm{CCE}^{+} 18\\right]\\) is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the \"Challenge\" version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves 51.4\\% accuracy in the zero-shot setting, \\(53.2 \\%\\) in the one-shot setting, and \\(51.5 \\%\\) in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline \\((55.9 \\%)\\) from UnifiedQA \\(\\left[\\mathrm{KKS}^{+} 20\\right]\\). On the \"Easy\" version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves \\(68.8 \\%, 71.2 \\%\\), and \\(70.1 \\%\\) which slightly exceeds a fine-tuned RoBERTa baseline from \\(\\left[\\mathrm{KKS}^{+} 20\\right]\\). However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3's few-shot results by \\(27 \\%\\) on the challenge set and \\(22 \\%\\) on the easy set. On OpenBookQA [MCKS18], GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3's few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard. Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3's performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset. GPT-3 performs best (within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC \\(\\left[\\mathrm{CHI}^{+}{ }^{18}\\right]\\) a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP [DWD \\(\\left.{ }^{+} 19\\right]\\), a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems [RLL+ 19]. On SQuAD 2.0 [RJL18], GPT-3 demonstrates its few-shot learning capabilities, improving by almost \\(10 \\mathrm{~F} 1\\) (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE [LXL+17], a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still \\(45 \\%\\) behind SOTA. 3.7 SuperGLUE In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark [WPN \\({ }^{+}\\)19] [WPN \\({ }^{+}\\)19] [CLC \\({ }^{+}\\)19] [DMST19] [RBG11] [KCR \\({ }^{+}\\)18] [ZLL \\({ }^{+}\\)18] [DGM06] [BHDD \\({ }^{+}\\)06] [GMDD07] \\(\\left[\\mathrm{BDD}^{+} 09\\right][\\mathrm{PCC} 18]\\left[\\mathrm{PHR}^{+}\\right.\\)18]. GPT-3's test-set performance on the SuperGLUE dataset is shown in Table 3.8. In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC 18"
            },
            {
                "id": "17",
                "metadata": {
                    "chunk_id": 17,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 17
                },
                "dis": 0.191725115675612,
                "documents": "\\begin{tabular}{lllll}\\hline Setting & PIQA & ARC (Easy) & ARC (Challenge) & OpenBookQA \\\\ \\hline Fine-tuned SOTA & 79.4 & \\(\\mathbf{9 2 . 0}\\left[\\mathrm{KKS}^{+} 20\\right]\\) & \\(\\mathbf{7 8 . 5}\\left[\\mathrm{KKS}^{+} 20\\right]\\) & \\(\\mathbf{8 7 . 2}\\left[\\mathrm{KKS}^{+} 20\\right]\\) \\\\ GPT-3 Zero-Shot & \\(\\mathbf{8 0 . 5}^{*}\\) & 68.8 & 51.4 & 57.6 \\\\ GPT-3 One-Shot & \\(\\mathbf{8 0 . 5}^{*}\\) & 71.2 & 53.2 & 58.8 \\\\ GPT-3 Few-Shot & \\(\\mathbf{8 2 . 8}^{*}\\) & 70.1 & 51.5 & 65.4 \\\\ \\hline\\end{tabular} Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set. Figure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task. such as the adversarially-mined Winogrande dataset [SBBC19] still significantly lag human performance. We test GPT-3's performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting. On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same \"partial evaluation\" method described in \\(\\left[\\mathrm{RWC}^{+} 19\\right]\\). Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves \\(88.3 \\%, 89.7 \\%\\), and \\(88.6 \\%\\) in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4). On the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves \\(70.2 \\%\\) in the zero-shot setting, \\(73.2 \\%\\) in the one-shot setting, and \\(77.7 \\%\\) in the few-shot setting. For comparison a fine-tuned RoBERTA model achieves 79\\%, state-of-the-art is \\(84.6 \\%\\) achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by [SBBC19] is \\(94.0 \\%\\). 3.5 Common Sense Reasoning Next we consider three datasets which attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering. The first, PhysicalQA (PIQA) [BZB \\({ }^{+}\\)19], asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves \\(81.0 \\%\\) accuracy zero-shot, \\(80.5 \\%\\) accuracy one-shot, and \\(82.8 \\%\\) accuracy few-shot (the last measured on PIQA's test server). This compares favorably to the \\(79.4 \\%\\) accuracy prior state-of-the-art of a 17"
            },
            {
                "id": "31",
                "metadata": {
                    "chunk_id": 31,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 31
                },
                "dis": 0.19216378032570347,
                "documents": "Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting. although models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent). GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared. We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn't feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results. For each benchmark, we produce a 'clean' version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C. We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over 50\\%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance. Below, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult. Our analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English 31"
            },
            {
                "id": "13",
                "metadata": {
                    "chunk_id": 13,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 13
                },
                "dis": 0.19236245823980402,
                "documents": "\\begin{tabular}{llll}\\hline Setting & NaturalQS & WebQS & TriviaQA \\\\ \\hline RAG (Fine-tuned, Open-Domain) [LPP \\({ }^{+}\\)20] & \\(\\mathbf{4 4 . 5}\\) & \\(\\mathbf{4 5 . 5}\\) & \\(\\mathbf{6 8 . 0}\\) \\\\ T5-11B+SSM (Fine-tuned, Closed-Book) [RRS20] & 36.6 & 44.7 & 60.5 \\\\ T5-11B (Fine-tuned, Closed-Book) & 34.5 & 37.4 & 50.1 \\\\ GPT-3 Zero-Shot & 14.6 & 14.4 & 64.3 \\\\ GPT-3 One-Shot & 23.0 & 25.3 & \\(\\mathbf{6 8 . 0}\\) \\\\ GPT-3 Few-Shot & 29.9 & 41.5 & \\(\\mathbf{7 1 . 2}\\) \\\\ \\hline\\end{tabular} Table 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server. One note of caution is that an analysis of test set contamination identified that a significant minority of the LAMBADA dataset appears to be present in our training data - however analysis performed in Section 4 suggests negligible impact on performance. 3.1.3 HellaSwag The HellaSwag dataset \\(\\left[\\mathrm{ZHB}^{+}\\right.\\)19] involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difficult for language models while remaining easy for humans (who achieve \\(95.6 \\%\\) accuracy). GPT-3 achieves \\(78.1 \\%\\) accuracy in the one-shot setting and 79.3\\% accuracy in the few-shot setting, outperforming the 75.4\\% accuracy of a fine-tuned 1.5B parameter language model [ \\(\\mathrm{ZHR}^{+}\\)19] but still a fair amount lower than the overall SOTA of \\(85.6 \\%\\) achieved by the fine-tuned multi-task model ALUM. 3.1.4 StoryCloze We next evaluate GPT- 3 on the StoryCloze 2016 dataset \\(\\left[\\mathrm{MCH}^{+}\\right.\\)16], which involves selecting the correct ending sentence for five-sentence long stories. Here GPT-3 achieves \\(83.2 \\%\\) in the zero-shot setting and \\(87.7 \\%\\) in the few-shot setting (with \\(K=70\\) ). This is still \\(4.1 \\%\\) lower than the fine-tuned SOTA using a BERT based model [LDL19] but improves over previous zero-shot results by roughly \\(10 \\%\\). 3.2 Closed Book Question Answering In this section we measure GPT-3's ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted \"open-book\". [RRS20] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as \"closed-book\". Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [RRS20]: Natural Questions [KPR \\({ }^{+}\\)19], WebQuestions [BCFL13], and TriviaQA [JCWZ17], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, fine-tuning on the Q\\&A dataset itself is also not permitted. The results for GPT-3 are shown in Table 3.3. On TriviaQA, we achieve \\(64.3 \\%\\) in the zero-shot setting, \\(68.0 \\%\\) in the one-shot setting, and \\(71.2 \\%\\) in the few-shot setting. The zero-shot result already outperforms the fine-tuned T5-11B by \\(14.2 \\%\\), and also outperforms a version with Q\\&A tailored span prediction during pre-training by 3.8\\%. The one-shot result improves by \\(3.7 \\%\\) and matches the SOTA for an open-domain QA system which not only fine-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of \\(21 \\mathrm{M}\\) documents [LPP \\({ }^{+} 20\\) ]. GPT-3's few-shot result further improves performance another \\(3.2 \\%\\) beyond this. On WebQuestions (WebQs), GPT-3 achieves 14.4\\% in the zero-shot setting, 25.3\\% in the one-shot setting, and 41.5\\% in the few-shot setting. This compares to \\(37.4 \\%\\) for fine-tuned T5-11B, and \\(44.7 \\%\\) for fine-tuned T5-11B+SSM, which uses a Q\\&A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions 13"
            },
            {
                "id": "33",
                "metadata": {
                    "chunk_id": 33,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 33
                },
                "dis": 0.193890756772075,
                "documents": "- Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children's Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark. We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within \\(0.5 \\%\\) of the full dataset. Also, strictly speaking, our fill-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section. An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inflates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing. Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C. 5 Limitations GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work. First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3's limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with \"common sense physics\", despite doing well on some datasets (such as PIQA [BZB \\(\\left.{ }^{+} 19\\right]\\) ) that test this domain. Specifically GPT-3 has difficulty with questions of the type \"If I put cheese into the fridge, will it melt?\". Quantitatively, GPT-3's in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some \"comparison\" tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3's strong few-shot performance on many other tasks. GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models \\(\\left[\\mathrm{RSR}^{+}\\right.\\)19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3's lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the \"best of both worlds\". A more fundamental limitation of the general approach described in this paper - scaling up any LM-like model, whether autoregressive or bidirectional - is that it may eventually run into (or could already be running into) the limits of the 33"
            },
            {
                "id": "16",
                "metadata": {
                    "chunk_id": 16,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 16
                },
                "dis": 0.19525880749732683,
                "documents": "\\begin{tabular}{lcc}\\hline Setting & Winograd & Winogrande (XL) \\\\ \\hline Fine-tuned SOTA & \\(\\mathbf{9 0 . 1}^{a}\\) & \\(\\mathbf{8 4 . 6}^{b}\\) \\\\ GPT-3 Zero-Shot & \\(88.3^{*}\\) & 70.2 \\\\ GPT-3 One-Shot & \\(89.7^{*}\\) & 73.2 \\\\ GPT-3 Few-Shot & \\(88.6^{*}\\) & 77.7 \\\\ \\hline\\end{tabular} Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set. \\({ }^{a}[\\mathrm{SBBC19}]{ }^{b}\\left[\\mathrm{LYN}^{+} 20\\right]\\) Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large. each translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation [LHCG19b]. Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H. 3.4 Winograd-Style Tasks The Winograd Schemas Challenge [LDM12] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions 16"
            },
            {
                "id": "1",
                "metadata": {
                    "chunk_id": 1,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 1
                },
                "dis": 0.19679015621831197,
                "documents": "Language Models are Few-Shot Learners \\(\\begin{array}{lll}\\text { Tom B. Brown* } & \\text { Benjamin Mann* } & \\text { Nick Ryder* }\\end{array}\\) Jared Kaplan \\({ }^{\\dagger}\\) Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M. Ziegler Jeffrey Wu Clemens Winter Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine- tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. \\({ }^{*}\\) Equal contribution \\({ }^{\\dagger}\\) Johns Hopkins University, OpenAI Author contributions listed at end of paper."
            },
            {
                "id": "12",
                "metadata": {
                    "chunk_id": 12,
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "page_id": 12
                },
                "dis": 0.1992399461612816,
                "documents": "\\begin{tabular}{lcccc}\\hline Setting & \\begin{tabular}{c} LAMBADA \\\\ \\((\\mathrm{acc})\\)\\end{tabular} & \\begin{tabular}{c} LAMBADA \\\\ \\((\\mathrm{ppl})\\)\\end{tabular} & \\begin{tabular}{c} StoryCloze \\\\ \\((\\mathrm{acc})\\)\\end{tabular} & \\begin{tabular}{c} HellaSwag \\\\ \\((\\mathrm{acc})\\)\\end{tabular} \\\\ \\hline SOTA & \\(68.0^{a}\\) & \\(8.63^{b}\\) & \\(\\mathbf{9 1 . 8}^{c}\\) & \\(\\mathbf{8 5 . 6}^{d}\\) \\\\ GPT-3 Zero-Shot & \\(\\mathbf{7 6 . 2}\\) & \\(\\mathbf{3 . 0 0}\\) & 83.2 & 78.9 \\\\ GPT-3 One-Shot & \\(\\mathbf{7 2 . 5}\\) & \\(\\mathbf{3 . 3 5}\\) & 84.7 & 78.1 \\\\ GPT-3 Few-Shot & \\(\\mathbf{8 6 . 4}\\) & \\(\\mathbf{1 . 9 2}\\) & 87.7 & 79.3 \\\\ \\hline\\end{tabular} Table 3.2: Performance on cloze and completion tasks. GPT-3 significantly improves SOTA on LAMBADA while achieving respectable performance on two difficult completion prediction datasets. \\({ }^{a}\\left[\\right.\\) Tur20] \\({ }^{b}\\left[\\mathrm{RWC}^{+}\\right.\\)19] \\({ }^{c}\\) [LDL19] \\({ }^{d}\\left[\\mathrm{LCH}^{+} 20\\right]\\) Figure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of the art by \\(18 \\%\\). Note zero-shot uses a different format from one-shot and few-shot as described in the text. and [Tur20]) and argue that \"continuing to expand hardware and data sizes by orders of magnitude is not the path forward\". We find that path is still promising and in a zero-shot setting GPT-3 achieves \\(76 \\%\\) on LAMBADA, a gain of \\(8 \\%\\) over the previous state of the art. LAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters \\(\\left[\\mathrm{RWC}^{+}{ }^{19]}\\right.\\) (which ban \"continuation\" words). The few-shot setting instead allows us to \"frame\" the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format: Alice was friends with Bob. Alice went to visit her friend \\(\\longrightarrow . \\rightarrow\\) Bob George bought some baseball equipment, a ball, a glove, and a \\(\\ldots . \\rightarrow\\) When presented with examples formatted this way, GPT-3 achieves \\(86.4 \\%\\) accuracy in the few-shot setting, an increase of over \\(18 \\%\\) from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost \\(20 \\%\\), for GPT-3 it improves accuracy by \\(10 \\%\\). Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern. 12"
            }
        ],
        "words": [
            {
                "id": "463",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3"
                },
                "dis": 0.043515563011169434,
                "documents": "GPT-3"
            },
            {
                "id": "461",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3 Medium"
                },
                "dis": 0.07378244400024414,
                "documents": "GPT-3 Medium"
            },
            {
                "id": "279",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3 XL"
                },
                "dis": 0.07540619373321533,
                "documents": "GPT-3 XL"
            },
            {
                "id": "485",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3 Large"
                },
                "dis": 0.07739609479904175,
                "documents": "GPT-3 Large"
            },
            {
                "id": "119",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3 Small"
                },
                "dis": 0.07826119661331177,
                "documents": "GPT-3 Small"
            },
            {
                "id": "136",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3 2.7B"
                },
                "dis": 0.09712439775466919,
                "documents": "GPT-3 2.7B"
            },
            {
                "id": "415",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3 13B"
                },
                "dis": 0.09812682867050171,
                "documents": "GPT-3 13B"
            },
            {
                "id": "307",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3 175B or GPT-3"
                },
                "dis": 0.10051745176315308,
                "documents": "GPT-3 175B or GPT-3"
            },
            {
                "id": "135",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "GPT-3 6.7B"
                },
                "dis": 0.10786217451095581,
                "documents": "GPT-3 6.7B"
            },
            {
                "id": "336",
                "metadata": {
                    "doc_id": "2024_03_29_a74940b61464fa47d70cg",
                    "word": "Additional Samples from GPT-3"
                },
                "dis": 0.14431971311569214,
                "documents": "Additional Samples from GPT-3"
            }
        ]
    },
    "request_id": "willamhou-search"
}